{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esraaelbaz/graduation_project/blob/main/O_HAZE_CANT_Haze_v10_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y104wnQRhV0i"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtgEF5xChScH",
        "outputId": "730074c8-b421-4a85-b7f1-ab2e5418acee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH4eeCUCWqCg"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY6shpbdCT-Y"
      },
      "outputs": [],
      "source": [
        "## To install pytorch 2.1.0 run this code block twice!\n",
        "# try:\n",
        "#   from torch import compile as t_compile\n",
        "# except:\n",
        "#   !pip install numpy --pre torch[dynamo] torchvision torchaudio --force-reinstall --extra-index-url https://download.pytorch.org/whl/nightly/cu117\n",
        "#   import os\n",
        "#   os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-41qUzAVWxXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d79ec3-63f1-4d6d-ce79-bdbcc69b22c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import save_image as imwrite\n",
        "\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "import random\n",
        "from math import exp, log10, ceil\n",
        "import gc\n",
        "\n",
        "try:\n",
        "  import einops\n",
        "  from einops import rearrange\n",
        "except:\n",
        "  !pip install einops\n",
        "  import einops\n",
        "  from einops import rearrange\n",
        "\n",
        "import os\n",
        "import gc\n",
        "\n",
        "try:\n",
        "  from torchmetrics import PeakSignalNoiseRatio as PSNR\n",
        "  from torchmetrics import StructuralSimilarityIndexMeasure as SSIM\n",
        "except:\n",
        "  !pip install torchmetrics\n",
        "  from torchmetrics import PeakSignalNoiseRatio as PSNR\n",
        "  from torchmetrics import StructuralSimilarityIndexMeasure as SSIM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fDqJ3Xc6oo1"
      },
      "source": [
        "# Patchify"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Size"
      ],
      "metadata": {
        "id": "QbwC6EjqVCnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_patchify(frame_in, crops_size, overlap_size):\n",
        "    patch_list = []\n",
        "    oversize_H = False\n",
        "    oversize_W = False\n",
        "    crops_size_H = crops_size[1]\n",
        "    crops_size_W = crops_size[0]\n",
        "    overlap_size_H = overlap_size[1]\n",
        "    overlap_size_W = overlap_size[0]\n",
        "    frame_in_H = frame_in.shape[-2]\n",
        "    frame_in_W = frame_in.shape[-1]\n",
        "    assert (crops_size_H >= 2 * overlap_size_H) and (crops_size_W >= 2 * overlap_size_W), \"Crops size should be at least 2x greater than overlap size\"\n",
        "    crops_per_row = 1\n",
        "    row_size = crops_size_H\n",
        "    while(row_size < frame_in_H):\n",
        "      # print(row_size)\n",
        "      row_size += (crops_size_H - overlap_size_H)\n",
        "      crops_per_row += 1\n",
        "\n",
        "    final_row_size = row_size - crops_size_H\n",
        "\n",
        "    crops_per_col = 1\n",
        "    col_size = crops_size_W\n",
        "    while(col_size < frame_in_W):\n",
        "      # print(col_size)\n",
        "      col_size += (crops_size_W - overlap_size_W)\n",
        "      crops_per_col += 1\n",
        "\n",
        "    final_col_size = col_size - crops_size_W\n",
        "\n",
        "    oversize_value_H = (int) (frame_in_H - final_row_size)\n",
        "    oversize_value_W = (int) (frame_in_W - final_col_size)\n",
        "    if (oversize_value_H != 0):\n",
        "      oversize_H = True\n",
        "    if (oversize_value_W != 0):\n",
        "      oversize_W = True\n",
        "\n",
        "    top = 0\n",
        "    height = crops_size_H\n",
        "    for i in range(crops_per_row):\n",
        "\n",
        "      left = 0\n",
        "      crop = []\n",
        "      if(i == (crops_per_row - 1)):\n",
        "        if (oversize_H == True):\n",
        "          height = oversize_value_H\n",
        "        else:\n",
        "          height = crops_size_H\n",
        "\n",
        "      for j in range(crops_per_col):\n",
        "\n",
        "        if((j != (crops_per_col - 1)) or oversize_W == False):\n",
        "          width = crops_size_W\n",
        "        elif(oversize_W == True):\n",
        "          width = oversize_value_W\n",
        "\n",
        "        # print(height, width)\n",
        "        crop.append(TF.crop(frame_in, top, left, height, width)) ##top , Left , Height , Width)\n",
        "        # print(\"LEFT\", j, left)\n",
        "        left += crops_size_W - overlap_size_W\n",
        "\n",
        "      patch_list.append(crop)\n",
        "      # print(\"TOP\" ,i, top)\n",
        "      top += crops_size_H - overlap_size_H\n",
        "\n",
        "    return patch_list, crops_per_row, crops_per_col\n",
        "\n",
        "def custom_unpatchify(patch_list, overlap_size, crops_per_row, crops_per_col):\n",
        "\n",
        "    overlap_size_H = overlap_size[1]\n",
        "    overlap_size_W = overlap_size[0]\n",
        "    unpatch_list = []\n",
        "    unpatch_list_W = []\n",
        "    unpatch_list_H = []\n",
        "    end_W = patch_list[0][0].shape[-1] - overlap_size_W\n",
        "    end_H = patch_list[0][0].shape[-2] - overlap_size_H\n",
        "\n",
        "    for i in range(crops_per_row):\n",
        "      crop_unpatch_list = []\n",
        "      for j in range(crops_per_col):\n",
        "        if(j == 0):\n",
        "          crop_W = 0\n",
        "        else:\n",
        "          crop_W = overlap_size_W\n",
        "\n",
        "        if(j != (crops_per_col - 1)):\n",
        "          crop_unpatch_list.append(patch_list[i][j][:, :, :, crop_W : end_W])\n",
        "          # print(patch_list[i][j].shape)\n",
        "        else:\n",
        "          crop_unpatch_list.append(patch_list[i][j][:, :, :, crop_W :])\n",
        "          # print(patch_list[i][j].shape)\n",
        "\n",
        "        if((j+1) < crops_per_col):\n",
        "          # print(patch_list[i][j][:, :, :,end_W : ].shape)\n",
        "          overlapping_area_W = (patch_list[i][j][:, :, :,end_W : ] + \\\n",
        "                                patch_list[i][j + 1][:, :, :,  : overlap_size_W]) / 2\n",
        "          # print(overlapping_area_W.shape)\n",
        "          crop_unpatch_list.append(overlapping_area_W)\n",
        "\n",
        "      # for k in range(len(crop_unpatch_list)):\n",
        "      #   print(crop_unpatch_list[k].shape[-2])\n",
        "      unpatch_list_W.append(torch.cat(crop_unpatch_list,-1))\n",
        "\n",
        "      if((i - 1) >= 0):\n",
        "        overlapping_area_H = (unpatch_list_W[i - 1][:, :, end_H :, :] + \\\n",
        "                              unpatch_list_W[i][:, :,  : overlap_size_H, :]) / 2\n",
        "        unpatch_list_H.append(overlapping_area_H)\n",
        "        if(i == 1):\n",
        "          unpatch_list_W[i - 1] = unpatch_list_W[i - 1][:, :, : end_H, :]\n",
        "\n",
        "        else:\n",
        "          unpatch_list_W[i - 1] = unpatch_list_W[i - 1][:, :, overlap_size_H : end_H, :]\n",
        "        # print(unpatch_list_W[i - 1].shape)\n",
        "      if(i == (crops_per_row - 1)):\n",
        "\n",
        "        unpatch_list_W[i] = unpatch_list_W[i][:, :, overlap_size_H :, :]\n",
        "        # print(unpatch_list_W[i].shape)\n",
        "\n",
        "    for z in range(len(unpatch_list_W)):\n",
        "      unpatch_list.append(unpatch_list_W[z])\n",
        "      if(z < len(unpatch_list_H)):\n",
        "        unpatch_list.append(unpatch_list_H[z])\n",
        "\n",
        "    frame_out = torch.cat(unpatch_list,-2)\n",
        "\n",
        "    return frame_out\n",
        "\n",
        "# ----------- testing the function ----------- #\n",
        "# crop_size = [384,384]\n",
        "# overlap_size = [100,100]\n",
        "# transform = transforms.ToTensor()\n",
        "# img = Image.open(Path(\"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/Test_GT/52_GT.png\")).convert(\"RGB\")\n",
        "# img_t = transform(img).unsqueeze(0)\n",
        "# print(img_t.shape)\n",
        "# transform = transforms.ToPILImage()\n",
        "# #transform(img_t.squeeze(0))\n",
        "\n",
        "# patch_list, crops_per_row, crops_per_col= custom_patchify(img_t, crop_size, overlap_size)\n",
        "# # print(crops_per_row, crops_per_col)\n",
        "# frameout=custom_unpatchify(patch_list, overlap_size, crops_per_row, crops_per_col)\n",
        "# print(frameout.shape)\n",
        "\n",
        "# transform(frameout.squeeze(0))"
      ],
      "metadata": {
        "id": "GR85wGMhVHhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc_EUNSWpnUB"
      },
      "source": [
        "## Grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQlNX1PS6nef"
      },
      "outputs": [],
      "source": [
        "def grid_patchify(frame_in,crops_amount):\n",
        "    patch_list_top = []\n",
        "    patch_list_bottom = []\n",
        "    crops_per_row = (int) (crops_amount / 2)\n",
        "    oversize = False\n",
        "    if(frame_in.shape[-2] % crops_per_row != 0):\n",
        "        oversize = True\n",
        "        oversize_value = (int) (frame_in.shape[-2] - (crops_per_row - 1) * (frame_in.shape[-2]//crops_per_row))\n",
        "    y = (int) (frame_in.shape[-2]//crops_per_row)\n",
        "    x = (int) (frame_in.shape[-1]//2)\n",
        "    for i in range(crops_per_row):\n",
        "        if(i != (crops_per_row - 1) or oversize == False):\n",
        "            w=TF.crop(frame_in, i*y, 0,  y,  x) ##top , Left , Height , Width\n",
        "            patch_list_top.append(w)\n",
        "            w=TF.crop(frame_in, i*y, x,  y,  x) ##top , Left , Height , Width\n",
        "            patch_list_bottom.append(w)\n",
        "        elif(oversize):\n",
        "            w=TF.crop(frame_in, i*y, 0,  oversize_value,  x) ##top , Left , Height , Width\n",
        "            patch_list_top.append(w)\n",
        "            w=TF.crop(frame_in, i*y, x,  oversize_value,  x) ##top , Left , Height , Width\n",
        "            patch_list_bottom.append(w)\n",
        "\n",
        "    return patch_list_top,patch_list_bottom\n",
        "\n",
        "def unpatchify(patch_list_T,patch_list_B):\n",
        "\n",
        "    frame_out_T=torch.cat(tuple(patch_list_T),-2)\n",
        "    # print(frame_out_T.shape)\n",
        "    frame_out_B=torch.cat(tuple(patch_list_B),-2)\n",
        "    # print(frame_out_B.shape)\n",
        "    frame_out = torch.cat((frame_out_T,frame_out_B),-1)\n",
        "    return frame_out\n",
        "\n",
        "# patch_list_T, patch_list_B=patchify(hazy,4)\n",
        "# frameout=unpatchify(patch_list_T,patch_list_B)\n",
        "# frameout.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmmteUSxpkiX"
      },
      "source": [
        "## Horizontal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-dCfhFpIJVL"
      },
      "outputs": [],
      "source": [
        "def horizontal_patchify(img,desiredpatchsize=512):\n",
        "    patches=[]\n",
        "    step=img.shape[1]//desiredpatchsize\n",
        "    stop=0\n",
        "    oversize = 0\n",
        "    for i in range (step):\n",
        "        patches.append(img[:,stop:(stop+desiredpatchsize),:])\n",
        "        stop+=desiredpatchsize\n",
        "\n",
        "    if img.shape[1]%desiredpatchsize != 0:\n",
        "        oversize= desiredpatchsize-(img.shape[1]%desiredpatchsize)\n",
        "        patches.append(img[:,(stop-oversize):((stop-oversize)+desiredpatchsize),:])\n",
        "    return patches, oversize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_-RV6anMq6l"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChQJP5sxMVJ6"
      },
      "outputs": [],
      "source": [
        "class CustomDataLoader(Dataset):\n",
        "    def __init__(self, HAZY_path = None, GT_path = None, image_size = (64,64), crop = False, resize = None):\n",
        "        self.HAZY_path = Path(HAZY_path)\n",
        "        self.GT_path = Path(GT_path)\n",
        "        self.HAZY_Image = []\n",
        "        self.GT_Image = []\n",
        "        for extension in ['png', 'jpg', 'JPG']:\n",
        "            self.HAZY_Image.extend(sorted(self.HAZY_path.glob('*.' + extension))) # list all the files present in HAZY images folder...\n",
        "            self.GT_Image.extend(sorted(self.GT_path.glob('*.' + extension))) # list all the files present in GT images folder...\n",
        "        self.HAZY_Image_Name = []\n",
        "\n",
        "        assert len(self.HAZY_Image) == len(self.GT_Image)\n",
        "        print(f\"Dataset has: {len(self.HAZY_Image)} images\")\n",
        "        self.crop = crop\n",
        "        self.resize = resize\n",
        "        if(self.crop):\n",
        "            self.train_transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                                                        transforms.TenCrop(image_size)])\n",
        "        elif(self.resize):\n",
        "            self.train_transforms = transforms.Compose([transforms.Resize(image_size),\n",
        "                                                        transforms.ToTensor()])\n",
        "        else:\n",
        "            self.train_transforms = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    def load_image(self, index: int, image_type = \"HAZY\") -> Image.Image:\n",
        "        \"Opens an image via a path and returns it.\"\n",
        "\n",
        "        if image_type == \"HAZY\":\n",
        "          image_path = self.HAZY_Image[index]\n",
        "\n",
        "        elif image_type == \"GT\":\n",
        "          image_path = self.GT_Image[index]\n",
        "\n",
        "        return Image.open(image_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.HAZY_Image) # return length of dataset\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #print(len(self.HAZY_Image),len(self.GT_image))\n",
        "        HAZY = Image.open(self.HAZY_Image[index]).convert(\"RGB\")\n",
        "        GT = Image.open(self.GT_Image[index]).convert(\"RGB\")\n",
        "        return self.train_transforms(HAZY), self.train_transforms(GT), self.HAZY_Image[index].stem\n",
        "\n",
        "#data augmentation for image rotate\n",
        "def custom_augment(hazy, clean):\n",
        "    augmentation_method = random.choice([0, 1, 2, 3, 4, 5])\n",
        "    rotate_degree = random.choice([90, 180, 270])\n",
        "    '''Rotate'''\n",
        "    if augmentation_method == 0:\n",
        "        hazy = transforms.functional.rotate(hazy, rotate_degree)\n",
        "        clean = transforms.functional.rotate(clean, rotate_degree)\n",
        "        return hazy, clean\n",
        "    '''Vertical'''\n",
        "    if augmentation_method == 1:\n",
        "        vertical_flip = torchvision.transforms.RandomVerticalFlip(p=1)\n",
        "        hazy = vertical_flip(hazy)\n",
        "        clean = vertical_flip(clean)\n",
        "        return hazy, clean\n",
        "    '''Horizontal'''\n",
        "    if augmentation_method == 2:\n",
        "        horizontal_flip = torchvision.transforms.RandomHorizontalFlip(p=1)\n",
        "        hazy = horizontal_flip(hazy)\n",
        "        clean = horizontal_flip(clean)\n",
        "        return hazy, clean\n",
        "    '''no change'''\n",
        "    if augmentation_method == 3 or augmentation_method == 4 or augmentation_method == 5:\n",
        "        return hazy, clean\n",
        "\n",
        "class custom_dehaze_train_dataset(Dataset):\n",
        "    def __init__(self, HAZY_path = None, GT_path = None, Image_Size = (256,256), is_train = True, random_crops = False, random_crop_sizes = [256, 512, 768]):\n",
        "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "        self.HAZY_path = Path(HAZY_path)\n",
        "        self.GT_path = Path(GT_path)\n",
        "        self.HAZY_Image = []\n",
        "        self.GT_Image = []\n",
        "        for extension in ['png', 'jpg', 'JPG']:\n",
        "            self.HAZY_Image.extend(sorted(self.HAZY_path.glob('*.' + extension))) # list all the files present in HAZY images folder...\n",
        "            self.GT_Image.extend(sorted(self.GT_path.glob('*.' + extension))) # list all the files present in GT images folder...\n",
        "        self.Image_Size = Image_Size\n",
        "        self.is_train = is_train\n",
        "        self.random_crops = random_crops\n",
        "        self.random_crop_sizes = random_crop_sizes\n",
        "        print(f\"Dataset has: {len(self.HAZY_Image)} images\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        hazy = Image.open(self.HAZY_Image[index]).convert(\"RGB\")\n",
        "        clean = Image.open(self.GT_Image[index]).convert(\"RGB\")\n",
        "        if self.is_train:\n",
        "            #crop a patch\n",
        "\n",
        "            if self.random_crops:\n",
        "                width = random.choice(self.random_crop_sizes)\n",
        "                height = random.choice(self.random_crop_sizes)\n",
        "                i,j,h,w = transforms.RandomCrop.get_params(hazy, output_size = (height, width))\n",
        "            else:\n",
        "                i,j,h,w = transforms.RandomCrop.get_params(hazy, output_size = self.Image_Size)\n",
        "            hazy_ = TF.crop(hazy, i, j, h, w)\n",
        "            clean_ = TF.crop(clean, i, j, h, w)\n",
        "\n",
        "            #data argumentation\n",
        "            hazy_arg, clean_arg = custom_augment(hazy_, clean_)\n",
        "            hazy = self.transform(hazy_arg)\n",
        "            clean = self.transform(clean_arg)\n",
        "            return hazy,clean\n",
        "        else:\n",
        "            hazy = self.transform(hazy)\n",
        "            clean = self.transform(clean)\n",
        "            return hazy,clean\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.HAZY_Image) # return length of dataset\n",
        "\n",
        "class dehaze_test_dataset(Dataset):\n",
        "    def __init__(self, HAZY_PATH = None, GT_PATH = None, Patchify = False):\n",
        "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "        self.root_hazy = Path(HAZY_PATH)\n",
        "        self.root_GT = Path(GT_PATH)\n",
        "        self.patchify = Patchify\n",
        "        self.list_test = []\n",
        "        self.list_GT = []\n",
        "        for extension in ['png', 'jpg', 'JPG']:\n",
        "            self.list_test.extend(sorted(self.root_hazy.glob('*.' + extension))) # list all the files present in HAZY images folder...\n",
        "            self.list_GT.extend(sorted(self.root_GT.glob('*.' + extension))) # list all the files present in GT images folder...\n",
        "        self.file_len = len(self.list_test)\n",
        "    def __getitem__(self, index, is_train=True):\n",
        "        hazy = Image.open(self.list_test[index]).convert(\"RGB\")\n",
        "        hazy = self.transform(hazy)\n",
        "        if(self.patchify):\n",
        "          hazy, oversize = horizontal_patchify(hazy)\n",
        "        name=self.list_test[index].stem\n",
        "        if (len(self.list_GT) == 0) and self.patchify:\n",
        "          return hazy, oversize, name\n",
        "        elif len(self.list_GT) == 0:\n",
        "          return hazy, name\n",
        "        else:\n",
        "          clean=Image.open(self.list_GT[index]).convert(\"RGB\")\n",
        "          clean = self.transform(clean)\n",
        "          clean = clean\n",
        "          if self.patchify:\n",
        "            return hazy, oversize, clean, name\n",
        "          else:\n",
        "            return hazy, clean, name\n",
        "    def __len__(self):\n",
        "        return self.file_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35Bzl6FB2gFf"
      },
      "source": [
        "# DWT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ECumqlC2tNX"
      },
      "source": [
        "## DWT functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9wKqmoU2smp"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) 2019, Adobe Inc. All rights reserved.\n",
        "#\n",
        "# This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike\n",
        "# 4.0 International Public License. To view a copy of this license, visit\n",
        "# https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode.\n",
        "\n",
        "\"\"\"\n",
        "自定义pytorch函数，实现一维、二维、三维张量的DWT和IDWT，未考虑边界延拓\n",
        "只有当图像行列数都是偶数，且重构滤波器组低频分量长度为2时，才能精确重构，否则在边界处有误差。\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch.autograd import Function\n",
        "\n",
        "\n",
        "class DWTFunction_1D(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, matrix_Low, matrix_High):\n",
        "        ctx.save_for_backward(matrix_Low, matrix_High)\n",
        "        L = torch.matmul(input, matrix_Low.t())\n",
        "        H = torch.matmul(input, matrix_High.t())\n",
        "        return L, H\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_L, grad_H):\n",
        "        matrix_L, matrix_H = ctx.saved_tensors\n",
        "        grad_input = torch.add(torch.matmul(\n",
        "            grad_L, matrix_L), torch.matmul(grad_H, matrix_H))\n",
        "        return grad_input, None, None\n",
        "\n",
        "\n",
        "class IDWTFunction_1D(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input_L, input_H, matrix_L, matrix_H):\n",
        "        ctx.save_for_backward(matrix_L, matrix_H)\n",
        "        output = torch.add(torch.matmul(input_L, matrix_L),\n",
        "                           torch.matmul(input_H, matrix_H))\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        matrix_L, matrix_H = ctx.saved_tensors\n",
        "        grad_L = torch.matmul(grad_output, matrix_L.t())\n",
        "        grad_H = torch.matmul(grad_output, matrix_H.t())\n",
        "        return grad_L, grad_H, None, None\n",
        "\n",
        "\n",
        "class DWTFunction_2D(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, matrix_Low_0, matrix_Low_1, matrix_High_0, matrix_High_1):\n",
        "        ctx.save_for_backward(matrix_Low_0, matrix_Low_1,\n",
        "                              matrix_High_0, matrix_High_1)\n",
        "        L = torch.matmul(matrix_Low_0, input)\n",
        "        H = torch.matmul(matrix_High_0, input)\n",
        "        LL = torch.matmul(L, matrix_Low_1)\n",
        "        LH = torch.matmul(L, matrix_High_1)\n",
        "        HL = torch.matmul(H, matrix_Low_1)\n",
        "        HH = torch.matmul(H, matrix_High_1)\n",
        "        return LL, LH, HL, HH\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_LL, grad_LH, grad_HL, grad_HH):\n",
        "        matrix_Low_0, matrix_Low_1, matrix_High_0, matrix_High_1 = ctx.saved_tensors\n",
        "        grad_L = torch.add(torch.matmul(grad_LL, matrix_Low_1.t()),\n",
        "                           torch.matmul(grad_LH, matrix_High_1.t()))\n",
        "        grad_H = torch.add(torch.matmul(grad_HL, matrix_Low_1.t()),\n",
        "                           torch.matmul(grad_HH, matrix_High_1.t()))\n",
        "        grad_input = torch.add(torch.matmul(\n",
        "            matrix_Low_0.t(), grad_L), torch.matmul(matrix_High_0.t(), grad_H))\n",
        "        return grad_input, None, None, None, None\n",
        "\n",
        "\n",
        "class DWTFunction_2D_tiny(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, matrix_Low_0, matrix_Low_1, matrix_High_0, matrix_High_1):\n",
        "        ctx.save_for_backward(matrix_Low_0, matrix_Low_1,\n",
        "                              matrix_High_0, matrix_High_1)\n",
        "        L = torch.matmul(matrix_Low_0, input)\n",
        "        LL = torch.matmul(L, matrix_Low_1)\n",
        "        return LL\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_LL):\n",
        "        matrix_Low_0, matrix_Low_1, matrix_High_0, matrix_High_1 = ctx.saved_tensors\n",
        "        grad_L = torch.matmul(grad_LL, matrix_Low_1.t())\n",
        "        grad_input = torch.matmul(matrix_Low_0.t(), grad_L)\n",
        "        return grad_input, None, None, None, None\n",
        "\n",
        "\n",
        "class IDWTFunction_2D(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input_LL, input_LH, input_HL, input_HH,\n",
        "                matrix_Low_0, matrix_Low_1, matrix_High_0, matrix_High_1):\n",
        "        ctx.save_for_backward(matrix_Low_0, matrix_Low_1,\n",
        "                              matrix_High_0, matrix_High_1)\n",
        "        L = torch.add(torch.matmul(input_LL, matrix_Low_1.t()),\n",
        "                      torch.matmul(input_LH, matrix_High_1.t()))\n",
        "        H = torch.add(torch.matmul(input_HL, matrix_Low_1.t()),\n",
        "                      torch.matmul(input_HH, matrix_High_1.t()))\n",
        "        output = torch.add(torch.matmul(matrix_Low_0.t(), L),\n",
        "                           torch.matmul(matrix_High_0.t(), H))\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        matrix_Low_0, matrix_Low_1, matrix_High_0, matrix_High_1 = ctx.saved_tensors\n",
        "        grad_L = torch.matmul(matrix_Low_0, grad_output)\n",
        "        grad_H = torch.matmul(matrix_High_0, grad_output)\n",
        "        grad_LL = torch.matmul(grad_L, matrix_Low_1)\n",
        "        grad_LH = torch.matmul(grad_L, matrix_High_1)\n",
        "        grad_HL = torch.matmul(grad_H, matrix_Low_1)\n",
        "        grad_HH = torch.matmul(grad_H, matrix_High_1)\n",
        "        return grad_LL, grad_LH, grad_HL, grad_HH, None, None, None, None\n",
        "\n",
        "\n",
        "class DWTFunction_3D(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input,\n",
        "                matrix_Low_0, matrix_Low_1, matrix_Low_2,\n",
        "                matrix_High_0, matrix_High_1, matrix_High_2):\n",
        "        ctx.save_for_backward(matrix_Low_0, matrix_Low_1, matrix_Low_2,\n",
        "                              matrix_High_0, matrix_High_1, matrix_High_2)\n",
        "        L = torch.matmul(matrix_Low_0, input)\n",
        "        H = torch.matmul(matrix_High_0, input)\n",
        "        LL = torch.matmul(L, matrix_Low_1).transpose(dim0=2, dim1=3)\n",
        "        LH = torch.matmul(L, matrix_High_1).transpose(dim0=2, dim1=3)\n",
        "        HL = torch.matmul(H, matrix_Low_1).transpose(dim0=2, dim1=3)\n",
        "        HH = torch.matmul(H, matrix_High_1).transpose(dim0=2, dim1=3)\n",
        "        LLL = torch.matmul(matrix_Low_2, LL).transpose(dim0=2, dim1=3)\n",
        "        LLH = torch.matmul(matrix_Low_2, LH).transpose(dim0=2, dim1=3)\n",
        "        LHL = torch.matmul(matrix_Low_2, HL).transpose(dim0=2, dim1=3)\n",
        "        LHH = torch.matmul(matrix_Low_2, HH).transpose(dim0=2, dim1=3)\n",
        "        HLL = torch.matmul(matrix_High_2, LL).transpose(dim0=2, dim1=3)\n",
        "        HLH = torch.matmul(matrix_High_2, LH).transpose(dim0=2, dim1=3)\n",
        "        HHL = torch.matmul(matrix_High_2, HL).transpose(dim0=2, dim1=3)\n",
        "        HHH = torch.matmul(matrix_High_2, HH).transpose(dim0=2, dim1=3)\n",
        "        return LLL, LLH, LHL, LHH, HLL, HLH, HHL, HHH\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_LLL, grad_LLH, grad_LHL, grad_LHH,\n",
        "                 grad_HLL, grad_HLH, grad_HHL, grad_HHH):\n",
        "        matrix_Low_0, matrix_Low_1, matrix_Low_2, matrix_High_0, matrix_High_1, matrix_High_2 = ctx.saved_tensors\n",
        "        grad_LL = torch.add(torch.matmul(matrix_Low_2.t(), grad_LLL.transpose(dim0=2, dim1=3)), torch.matmul(\n",
        "            matrix_High_2.t(), grad_HLL.transpose(dim0=2, dim1=3))).transpose(dim0=2, dim1=3)\n",
        "        grad_LH = torch.add(torch.matmul(matrix_Low_2.t(), grad_LLH.transpose(dim0=2, dim1=3)), torch.matmul(\n",
        "            matrix_High_2.t(), grad_HLH.transpose(dim0=2, dim1=3))).transpose(dim0=2, dim1=3)\n",
        "        grad_HL = torch.add(torch.matmul(matrix_Low_2.t(), grad_LHL.transpose(dim0=2, dim1=3)), torch.matmul(\n",
        "            matrix_High_2.t(), grad_HHL.transpose(dim0=2, dim1=3))).transpose(dim0=2, dim1=3)\n",
        "        grad_HH = torch.add(torch.matmul(matrix_Low_2.t(), grad_LHH.transpose(dim0=2, dim1=3)), torch.matmul(\n",
        "            matrix_High_2.t(), grad_HHH.transpose(dim0=2, dim1=3))).transpose(dim0=2, dim1=3)\n",
        "        grad_L = torch.add(torch.matmul(grad_LL, matrix_Low_1.t()),\n",
        "                           torch.matmul(grad_LH, matrix_High_1.t()))\n",
        "        grad_H = torch.add(torch.matmul(grad_HL, matrix_Low_1.t()),\n",
        "                           torch.matmul(grad_HH, matrix_High_1.t()))\n",
        "        grad_input = torch.add(torch.matmul(\n",
        "            matrix_Low_0.t(), grad_L), torch.matmul(matrix_High_0.t(), grad_H))\n",
        "        return grad_input, None, None, None, None, None, None, None, None\n",
        "\n",
        "\n",
        "class IDWTFunction_3D(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input_LLL, input_LLH, input_LHL, input_LHH,\n",
        "                input_HLL, input_HLH, input_HHL, input_HHH,\n",
        "                matrix_Low_0, matrix_Low_1, matrix_Low_2,\n",
        "                matrix_High_0, matrix_High_1, matrix_High_2):\n",
        "        ctx.save_for_backward(matrix_Low_0, matrix_Low_1, matrix_Low_2,\n",
        "                              matrix_High_0, matrix_High_1, matrix_High_2)\n",
        "        input_LL = torch.add(torch.matmul(matrix_Low_2.t(), input_LLL.transpose(dim0=2, dim1=3)), torch.matmul(\n",
        "            matrix_High_2.t(), input_HLL.transpose(dim0=2, dim1=3))).transpose(dim0=2, dim1=3)\n",
        "        input_LH = torch.add(torch.matmul(matrix_Low_2.t(), input_LLH.transpose(dim0=2, dim1=3)), torch.matmul(\n",
        "            matrix_High_2.t(), input_HLH.transpose(dim0=2, dim1=3))).transpose(dim0=2, dim1=3)\n",
        "        input_HL = torch.add(torch.matmul(matrix_Low_2.t(), input_LHL.transpose(dim0=2, dim1=3)), torch.matmul(\n",
        "            matrix_High_2.t(), input_HHL.transpose(dim0=2, dim1=3))).transpose(dim0=2, dim1=3)\n",
        "        input_HH = torch.add(torch.matmul(matrix_Low_2.t(), input_LHH.transpose(dim0=2, dim1=3)), torch.matmul(\n",
        "            matrix_High_2.t(), input_HHH.transpose(dim0=2, dim1=3))).transpose(dim0=2, dim1=3)\n",
        "        input_L = torch.add(torch.matmul(input_LL, matrix_Low_1.t()),\n",
        "                            torch.matmul(input_LH, matrix_High_1.t()))\n",
        "        input_H = torch.add(torch.matmul(input_HL, matrix_Low_1.t()),\n",
        "                            torch.matmul(input_HH, matrix_High_1.t()))\n",
        "        output = torch.add(torch.matmul(matrix_Low_0.t(), input_L),\n",
        "                           torch.matmul(matrix_High_0.t(), input_H))\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        matrix_Low_0, matrix_Low_1, matrix_Low_2, matrix_High_0, matrix_High_1, matrix_High_2 = ctx.saved_tensors\n",
        "        grad_L = torch.matmul(matrix_Low_0, grad_output)\n",
        "        grad_H = torch.matmul(matrix_High_0, grad_output)\n",
        "        grad_LL = torch.matmul(grad_L, matrix_Low_1).transpose(dim0=2, dim1=3)\n",
        "        grad_LH = torch.matmul(grad_L, matrix_High_1).transpose(dim0=2, dim1=3)\n",
        "        grad_HL = torch.matmul(grad_H, matrix_Low_1).transpose(dim0=2, dim1=3)\n",
        "        grad_HH = torch.matmul(grad_H, matrix_High_1).transpose(dim0=2, dim1=3)\n",
        "        grad_LLL = torch.matmul(\n",
        "            matrix_Low_2, grad_LL).transpose(dim0=2, dim1=3)\n",
        "        grad_LLH = torch.matmul(\n",
        "            matrix_Low_2, grad_LH).transpose(dim0=2, dim1=3)\n",
        "        grad_LHL = torch.matmul(\n",
        "            matrix_Low_2, grad_HL).transpose(dim0=2, dim1=3)\n",
        "        grad_LHH = torch.matmul(\n",
        "            matrix_Low_2, grad_HH).transpose(dim0=2, dim1=3)\n",
        "        grad_HLL = torch.matmul(\n",
        "            matrix_High_2, grad_LL).transpose(dim0=2, dim1=3)\n",
        "        grad_HLH = torch.matmul(\n",
        "            matrix_High_2, grad_LH).transpose(dim0=2, dim1=3)\n",
        "        grad_HHL = torch.matmul(\n",
        "            matrix_High_2, grad_HL).transpose(dim0=2, dim1=3)\n",
        "        grad_HHH = torch.matmul(\n",
        "            matrix_High_2, grad_HH).transpose(dim0=2, dim1=3)\n",
        "        return grad_LLL, grad_LLH, grad_LHL, grad_LHH, grad_HLL, grad_HLH, grad_HHL, grad_HHH, None, None, None, None, None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kzgsmWy2oOE"
      },
      "source": [
        "## DWT Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4qSHY7u2h60"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "自定义 pytorch 层，实现一维、二维、三维张量的 DWT 和 IDWT，未考虑边界延拓\n",
        "只有当图像行列数都是偶数，且重构滤波器组低频分量长度为 2 时，才能精确重构，否则在边界处有误差。\n",
        "\"\"\"\n",
        "import math\n",
        "# import wave\n",
        "\n",
        "import numpy as np\n",
        "import pywt\n",
        "import torch\n",
        "from torch.nn import Module\n",
        "\n",
        "# from .DWT_IDWT_Functions import DWTFunction_1D, IDWTFunction_1D, \\\n",
        "#     DWTFunction_2D_tiny, DWTFunction_2D, IDWTFunction_2D, \\\n",
        "#     DWTFunction_3D, IDWTFunction_3D\n",
        "\n",
        "\n",
        "__all__ = ['DWT_1D', 'IDWT_1D', 'DWT_2D',\n",
        "           'IDWT_2D', 'DWT_3D', 'IDWT_3D', 'DWT_2D_tiny']\n",
        "\n",
        "\n",
        "class DWT_1D(Module):\n",
        "    \"\"\"\n",
        "    input: the 1D data to be decomposed -- (N, C, Length)\n",
        "    output: lfc -- (N, C, Length/2)\n",
        "            hfc -- (N, C, Length/2)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wavename):\n",
        "        \"\"\"\n",
        "        1D discrete wavelet transform (DWT) for sequence decomposition\n",
        "        用于序列分解的一维离散小波变换 DWT\n",
        "        :param wavename: pywt.wavelist(); in the paper, 'chx.y' denotes 'biorx.y'.\n",
        "        \"\"\"\n",
        "        super(DWT_1D, self).__init__()\n",
        "        wavelet = pywt.Wavelet(wavename)\n",
        "        self.band_low = wavelet.rec_lo\n",
        "        self.band_high = wavelet.rec_hi\n",
        "        assert len(self.band_low) == len(self.band_high)\n",
        "        self.band_length = len(self.band_low)\n",
        "        assert self.band_length % 2 == 0\n",
        "        self.band_length_half = math.floor(self.band_length / 2)\n",
        "\n",
        "    def get_matrix(self):\n",
        "        \"\"\"\n",
        "        生成变换矩阵\n",
        "        generating the matrices: \\mathcal{L}, \\mathcal{H}\n",
        "        :return: self.matrix_low = \\mathcal{L}, self.matrix_high = \\mathcal{H}\n",
        "        \"\"\"\n",
        "        L1 = self.input_height\n",
        "        L = math.floor(L1 / 2)\n",
        "        matrix_h = np.zeros((L, L1 + self.band_length - 2))\n",
        "        matrix_g = np.zeros((L1 - L, L1 + self.band_length - 2))\n",
        "        end = None if self.band_length_half == 1 else (\n",
        "            - self.band_length_half + 1)\n",
        "        index = 0\n",
        "        for i in range(L):\n",
        "            for j in range(self.band_length):\n",
        "                matrix_h[i, index + j] = self.band_low[j]\n",
        "            index += 2\n",
        "        index = 0\n",
        "        for i in range(L1 - L):\n",
        "            for j in range(self.band_length):\n",
        "                matrix_g[i, index + j] = self.band_high[j]\n",
        "            index += 2\n",
        "        matrix_h = matrix_h[:, (self.band_length_half - 1):end]\n",
        "        matrix_g = matrix_g[:, (self.band_length_half - 1):end]\n",
        "        if torch.cuda.is_available():\n",
        "            self.matrix_low = torch.Tensor(matrix_h).cuda()\n",
        "            self.matrix_high = torch.Tensor(matrix_g).cuda()\n",
        "        else:\n",
        "            self.matrix_low = torch.Tensor(matrix_h)\n",
        "            self.matrix_high = torch.Tensor(matrix_g)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        input_low_frequency_component = \\mathcal{L} * input\n",
        "        input_high_frequency_component = \\mathcal{H} * input\n",
        "        :param input: the data to be decomposed\n",
        "        :return: the low-frequency and high-frequency components of the input data\n",
        "        \"\"\"\n",
        "        assert len(input.size()) == 3\n",
        "        self.input_height = input.size()[-1]\n",
        "        self.get_matrix()\n",
        "        return DWTFunction_1D.apply(input, self.matrix_low, self.matrix_high)\n",
        "\n",
        "\n",
        "class IDWT_1D(Module):\n",
        "    \"\"\"\n",
        "    input:  lfc -- (N, C, Length/2)\n",
        "            hfc -- (N, C, Length/2)\n",
        "    output: the original data -- (N, C, Length)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wavename):\n",
        "        \"\"\"\n",
        "        1D inverse DWT (IDWT) for sequence reconstruction\n",
        "        用于序列重构的一维离散小波逆变换 IDWT\n",
        "        :param wavename: pywt.wavelist(); in the paper, 'chx.y' denotes 'biorx.y'.\n",
        "        \"\"\"\n",
        "        super(IDWT_1D, self).__init__()\n",
        "        wavelet = pywt.Wavelet(wavename)\n",
        "        self.band_low = wavelet.dec_lo\n",
        "        self.band_high = wavelet.dec_hi\n",
        "        self.band_low.reverse()\n",
        "        self.band_high.reverse()\n",
        "        assert len(self.band_low) == len(self.band_high)\n",
        "        self.band_length = len(self.band_low)\n",
        "        assert self.band_length % 2 == 0\n",
        "        self.band_length_half = math.floor(self.band_length / 2)\n",
        "\n",
        "    def get_matrix(self):\n",
        "        \"\"\"\n",
        "        generating the matrices: \\mathcal{L}, \\mathcal{H}\n",
        "        生成变换矩阵\n",
        "        :return: self.matrix_low = \\mathcal{L}, self.matrix_high = \\mathcal{H}\n",
        "        \"\"\"\n",
        "        L1 = self.input_height\n",
        "        L = math.floor(L1 / 2)\n",
        "        matrix_h = np.zeros((L, L1 + self.band_length - 2))\n",
        "        matrix_g = np.zeros((L1 - L, L1 + self.band_length - 2))\n",
        "        end = None if self.band_length_half == 1 else (\n",
        "            - self.band_length_half + 1)\n",
        "        index = 0\n",
        "        for i in range(L):\n",
        "            for j in range(self.band_length):\n",
        "                matrix_h[i, index + j] = self.band_low[j]\n",
        "            index += 2\n",
        "        index = 0\n",
        "        for i in range(L1 - L):\n",
        "            for j in range(self.band_length):\n",
        "                matrix_g[i, index + j] = self.band_high[j]\n",
        "            index += 2\n",
        "        matrix_h = matrix_h[:, (self.band_length_half - 1):end]\n",
        "        matrix_g = matrix_g[:, (self.band_length_half - 1):end]\n",
        "        if torch.cuda.is_available():\n",
        "            self.matrix_low = torch.Tensor(matrix_h).cuda()\n",
        "            self.matrix_high = torch.Tensor(matrix_g).cuda()\n",
        "        else:\n",
        "            self.matrix_low = torch.Tensor(matrix_h)\n",
        "            self.matrix_high = torch.Tensor(matrix_g)\n",
        "\n",
        "    def forward(self, L, H):\n",
        "        \"\"\"\n",
        "        :param L: the low-frequency component of the original data\n",
        "        :param H: the high-frequency component of the original data\n",
        "        :return: the original data\n",
        "        \"\"\"\n",
        "        assert len(L.size()) == len(H.size()) == 3\n",
        "        self.input_height = L.size()[-1] + H.size()[-1]\n",
        "        self.get_matrix()\n",
        "        return IDWTFunction_1D.apply(L, H, self.matrix_low, self.matrix_high)\n",
        "\n",
        "\n",
        "class DWT_2D_tiny(Module):\n",
        "    \"\"\"\n",
        "    input: the 2D data to be decomposed -- (N, C, H, W)\n",
        "    output -- lfc: (N, C, H/2, W/2)\n",
        "              #hfc_lh: (N, C, H/2, W/2)\n",
        "              #hfc_hl: (N, C, H/2, W/2)\n",
        "              #hfc_hh: (N, C, H/2, W/2)\n",
        "    DWT_2D_tiny only outputs the low-frequency component, which is used in WaveCNet;\n",
        "    the all four components could be get using DWT_2D, which is used in WaveUNet.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wavename):\n",
        "        \"\"\"\n",
        "        2D discrete wavelet transform (DWT) for 2D image decomposition\n",
        "        :param wavename: pywt.wavelist(); in the paper, 'chx.y' denotes 'biorx.y'.\n",
        "        \"\"\"\n",
        "        super(DWT_2D_tiny, self).__init__()\n",
        "        wavelet = pywt.Wavelet(wavename)\n",
        "        self.band_low = wavelet.rec_lo\n",
        "        self.band_high = wavelet.rec_hi\n",
        "        assert len(self.band_low) == len(self.band_high)\n",
        "        self.band_length = len(self.band_low)\n",
        "        assert self.band_length % 2 == 0\n",
        "        self.band_length_half = math.floor(self.band_length / 2)\n",
        "\n",
        "    def get_matrix(self):\n",
        "        \"\"\"\n",
        "        生成变换矩阵\n",
        "        generating the matrices: \\mathcal{L}, \\mathcal{H}\n",
        "        :return: self.matrix_low = \\mathcal{L}, self.matrix_high = \\mathcal{H}\n",
        "        \"\"\"\n",
        "        L1 = np.max((self.input_height, self.input_width))\n",
        "        L = math.floor(L1 / 2)\n",
        "        matrix_h = np.zeros((L, L1 + self.band_length - 2))\n",
        "        matrix_g = np.zeros((L1 - L, L1 + self.band_length - 2))\n",
        "        end = None if self.band_length_half == 1 else (\n",
        "            - self.band_length_half + 1)\n",
        "\n",
        "        index = 0\n",
        "        for i in range(L):\n",
        "            for j in range(self.band_length):\n",
        "                matrix_h[i, index + j] = self.band_low[j]\n",
        "            index += 2\n",
        "        matrix_h_0 = matrix_h[0:(math.floor(\n",
        "            self.input_height / 2)), 0:(self.input_height + self.band_length - 2)]\n",
        "        matrix_h_1 = matrix_h[0:(math.floor(\n",
        "            self.input_width / 2)), 0:(self.input_width + self.band_length - 2)]\n",
        "\n",
        "        index = 0\n",
        "        for i in range(L1 - L):\n",
        "            for j in range(self.band_length):\n",
        "                matrix_g[i, index + j] = self.band_high[j]\n",
        "            index += 2\n",
        "        matrix_g_0 = matrix_g[0:(self.input_height - math.floor(\n",
        "            self.input_height / 2)), 0:(self.input_height + self.band_length - 2)]\n",
        "        matrix_g_1 = matrix_g[0:(self.input_width - math.floor(\n",
        "            self.input_width / 2)), 0:(self.input_width + self.band_length - 2)]\n",
        "\n",
        "        matrix_h_0 = matrix_h_0[:, (self.band_length_half - 1):end]\n",
        "        matrix_h_1 = matrix_h_1[:, (self.band_length_half - 1):end]\n",
        "        matrix_h_1 = np.transpose(matrix_h_1)\n",
        "        matrix_g_0 = matrix_g_0[:, (self.band_length_half - 1):end]\n",
        "        matrix_g_1 = matrix_g_1[:, (self.band_length_half - 1):end]\n",
        "        matrix_g_1 = np.transpose(matrix_g_1)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.matrix_low_0 = torch.Tensor(matrix_h_0).cuda()\n",
        "            self.matrix_low_1 = torch.Tensor(matrix_h_1).cuda()\n",
        "            self.matrix_high_0 = torch.Tensor(matrix_g_0).cuda()\n",
        "            self.matrix_high_1 = torch.Tensor(matrix_g_1).cuda()\n",
        "        else:\n",
        "            self.matrix_low_0 = torch.Tensor(matrix_h_0)\n",
        "            self.matrix_low_1 = torch.Tensor(matrix_h_1)\n",
        "            self.matrix_high_0 = torch.Tensor(matrix_g_0)\n",
        "            self.matrix_high_1 = torch.Tensor(matrix_g_1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        input_lfc = \\mathcal{L} * input * \\mathcal{L}^T\n",
        "        #input_hfc_lh = \\mathcal{H} * input * \\mathcal{L}^T\n",
        "        #input_hfc_hl = \\mathcal{L} * input * \\mathcal{H}^T\n",
        "        #input_hfc_hh = \\mathcal{H} * input * \\mathcal{H}^T\n",
        "        :param input: the 2D data to be decomposed\n",
        "        :return: the low-frequency component of the input 2D data\n",
        "        \"\"\"\n",
        "        assert len(input.size()) == 4\n",
        "        self.input_height = input.size()[-2]\n",
        "        self.input_width = input.size()[-1]\n",
        "        self.get_matrix()\n",
        "        return DWTFunction_2D_tiny.apply(input, self.matrix_low_0, self.matrix_low_1, self.matrix_high_0, self.matrix_high_1)\n",
        "\n",
        "\n",
        "class DWT_2D(Module):\n",
        "    \"\"\"\n",
        "    input: the 2D data to be decomposed -- (N, C, H, W)\n",
        "    output -- lfc: (N, C, H/2, W/2)\n",
        "              hfc_lh: (N, C, H/2, W/2)\n",
        "              hfc_hl: (N, C, H/2, W/2)\n",
        "              hfc_hh: (N, C, H/2, W/2)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wavename):\n",
        "        \"\"\"\n",
        "        2D discrete wavelet transform (DWT) for 2D image decomposition\n",
        "        :param wavename: pywt.wavelist(); in the paper, 'chx.y' denotes 'biorx.y'.\n",
        "        \"\"\"\n",
        "        super(DWT_2D, self).__init__()\n",
        "        wavelet = pywt.Wavelet(wavename)\n",
        "        self.band_low = wavelet.rec_lo\n",
        "        self.band_high = wavelet.rec_hi\n",
        "        assert len(self.band_low) == len(self.band_high)\n",
        "        self.band_length = len(self.band_low)\n",
        "        assert self.band_length % 2 == 0\n",
        "        self.band_length_half = math.floor(self.band_length / 2)\n",
        "\n",
        "    def get_matrix(self):\n",
        "        \"\"\"\n",
        "        生成变换矩阵\n",
        "        generating the matrices: \\mathcal{L}, \\mathcal{H}\n",
        "        :return: self.matrix_low = \\mathcal{L}, self.matrix_high = \\mathcal{H}\n",
        "        \"\"\"\n",
        "        L1 = np.max((self.input_height, self.input_width))\n",
        "        L = math.floor(L1 / 2)\n",
        "        matrix_h = np.zeros((L, L1 + self.band_length - 2))\n",
        "        matrix_g = np.zeros((L1 - L, L1 + self.band_length - 2))\n",
        "        end = None if self.band_length_half == 1 else (\n",
        "            - self.band_length_half + 1)\n",
        "\n",
        "        index = 0\n",
        "        for i in range(L):\n",
        "            for j in range(self.band_length):\n",
        "                matrix_h[i, index + j] = self.band_low[j]\n",
        "            index += 2\n",
        "        matrix_h_0 = matrix_h[0:(math.floor(\n",
        "            self.input_height / 2)), 0:(self.input_height + self.band_length - 2)]\n",
        "        matrix_h_1 = matrix_h[0:(math.floor(\n",
        "            self.input_width / 2)), 0:(self.input_width + self.band_length - 2)]\n",
        "\n",
        "        index = 0\n",
        "        for i in range(L1 - L):\n",
        "            for j in range(self.band_length):\n",
        "                matrix_g[i, index + j] = self.band_high[j]\n",
        "            index += 2\n",
        "        matrix_g_0 = matrix_g[0:(self.input_height - math.floor(\n",
        "            self.input_height / 2)), 0:(self.input_height + self.band_length - 2)]\n",
        "        matrix_g_1 = matrix_g[0:(self.input_width - math.floor(\n",
        "            self.input_width / 2)), 0:(self.input_width + self.band_length - 2)]\n",
        "\n",
        "        matrix_h_0 = matrix_h_0[:, (self.band_length_half - 1):end]\n",
        "        matrix_h_1 = matrix_h_1[:, (self.band_length_half - 1):end]\n",
        "        matrix_h_1 = np.transpose(matrix_h_1)\n",
        "        matrix_g_0 = matrix_g_0[:, (self.band_length_half - 1):end]\n",
        "        matrix_g_1 = matrix_g_1[:, (self.band_length_half - 1):end]\n",
        "        matrix_g_1 = np.transpose(matrix_g_1)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.matrix_low_0 = torch.Tensor(matrix_h_0).cuda()\n",
        "            self.matrix_low_1 = torch.Tensor(matrix_h_1).cuda()\n",
        "            self.matrix_high_0 = torch.Tensor(matrix_g_0).cuda()\n",
        "            self.matrix_high_1 = torch.Tensor(matrix_g_1).cuda()\n",
        "        else:\n",
        "            self.matrix_low_0 = torch.Tensor(matrix_h_0)\n",
        "            self.matrix_low_1 = torch.Tensor(matrix_h_1)\n",
        "            self.matrix_high_0 = torch.Tensor(matrix_g_0)\n",
        "            self.matrix_high_1 = torch.Tensor(matrix_g_1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        input_lfc = \\mathcal{L} * input * \\mathcal{L}^T\n",
        "        input_hfc_lh = \\mathcal{H} * input * \\mathcal{L}^T\n",
        "        input_hfc_hl = \\mathcal{L} * input * \\mathcal{H}^T\n",
        "        input_hfc_hh = \\mathcal{H} * input * \\mathcal{H}^T\n",
        "        :param input: the 2D data to be decomposed\n",
        "        :return: the low-frequency and high-frequency components of the input 2D data\n",
        "        \"\"\"\n",
        "        assert len(input.size()) == 4\n",
        "        self.input_height = input.size()[-2]\n",
        "        self.input_width = input.size()[-1]\n",
        "        self.get_matrix()\n",
        "        return DWTFunction_2D.apply(input, self.matrix_low_0, self.matrix_low_1, self.matrix_high_0, self.matrix_high_1)\n",
        "\n",
        "\n",
        "class IDWT_2D(Module):\n",
        "    \"\"\"\n",
        "    input:  lfc -- (N, C, H/2, W/2)\n",
        "            hfc_lh -- (N, C, H/2, W/2)\n",
        "            hfc_hl -- (N, C, H/2, W/2)\n",
        "            hfc_hh -- (N, C, H/2, W/2)\n",
        "    output: the original 2D data -- (N, C, H, W)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wavename):\n",
        "        \"\"\"\n",
        "        2D inverse DWT (IDWT) for 2D image reconstruction\n",
        "        :param wavename: pywt.wavelist(); in the paper, 'chx.y' denotes 'biorx.y'.\n",
        "        \"\"\"\n",
        "        super(IDWT_2D, self).__init__()\n",
        "        wavelet = pywt.Wavelet(wavename)\n",
        "        self.band_low = wavelet.dec_lo\n",
        "        self.band_low.reverse()\n",
        "        self.band_high = wavelet.dec_hi\n",
        "        self.band_high.reverse()\n",
        "        assert len(self.band_low) == len(self.band_high)\n",
        "        self.band_length = len(self.band_low)\n",
        "        assert self.band_length % 2 == 0\n",
        "        self.band_length_half = math.floor(self.band_length / 2)\n",
        "\n",
        "    def get_matrix(self):\n",
        "        \"\"\"\n",
        "        生成变换矩阵\n",
        "        generating the matrices: \\mathcal{L}, \\mathcal{H}\n",
        "        :return: self.matrix_low = \\mathcal{L}, self.matrix_high = \\mathcal{H}\n",
        "        \"\"\"\n",
        "        L1 = np.max((self.input_height, self.input_width))\n",
        "        L = math.floor(L1 / 2)\n",
        "        matrix_h = np.zeros((L, L1 + self.band_length - 2))\n",
        "        matrix_g = np.zeros((L1 - L, L1 + self.band_length - 2))\n",
        "        end = None if self.band_length_half == 1 else (\n",
        "            - self.band_length_half + 1)\n",
        "\n",
        "        index = 0\n",
        "        for i in range(L):\n",
        "            for j in range(self.band_length):\n",
        "                matrix_h[i, index + j] = self.band_low[j]\n",
        "            index += 2\n",
        "        matrix_h_0 = matrix_h[0:(math.floor(\n",
        "            self.input_height / 2)), 0:(self.input_height + self.band_length - 2)]\n",
        "        matrix_h_1 = matrix_h[0:(math.floor(\n",
        "            self.input_width / 2)), 0:(self.input_width + self.band_length - 2)]\n",
        "\n",
        "        index = 0\n",
        "        for i in range(L1 - L):\n",
        "            for j in range(self.band_length):\n",
        "                matrix_g[i, index + j] = self.band_high[j]\n",
        "            index += 2\n",
        "        matrix_g_0 = matrix_g[0:(self.input_height - math.floor(\n",
        "            self.input_height / 2)), 0:(self.input_height + self.band_length - 2)]\n",
        "        matrix_g_1 = matrix_g[0:(self.input_width - math.floor(\n",
        "            self.input_width / 2)), 0:(self.input_width + self.band_length - 2)]\n",
        "\n",
        "        matrix_h_0 = matrix_h_0[:, (self.band_length_half - 1):end]\n",
        "        matrix_h_1 = matrix_h_1[:, (self.band_length_half - 1):end]\n",
        "        matrix_h_1 = np.transpose(matrix_h_1)\n",
        "        matrix_g_0 = matrix_g_0[:, (self.band_length_half - 1):end]\n",
        "        matrix_g_1 = matrix_g_1[:, (self.band_length_half - 1):end]\n",
        "        matrix_g_1 = np.transpose(matrix_g_1)\n",
        "        if torch.cuda.is_available():\n",
        "            self.matrix_low_0 = torch.Tensor(matrix_h_0).cuda()\n",
        "            self.matrix_low_1 = torch.Tensor(matrix_h_1).cuda()\n",
        "            self.matrix_high_0 = torch.Tensor(matrix_g_0).cuda()\n",
        "            self.matrix_high_1 = torch.Tensor(matrix_g_1).cuda()\n",
        "        else:\n",
        "            self.matrix_low_0 = torch.Tensor(matrix_h_0)\n",
        "            self.matrix_low_1 = torch.Tensor(matrix_h_1)\n",
        "            self.matrix_high_0 = torch.Tensor(matrix_g_0)\n",
        "            self.matrix_high_1 = torch.Tensor(matrix_g_1)\n",
        "\n",
        "    def forward(self, LL, LH, HL, HH):\n",
        "        \"\"\"\n",
        "        recontructing the original 2D data\n",
        "        the original 2D data = \\mathcal{L}^T * lfc * \\mathcal{L}\n",
        "                             + \\mathcal{H}^T * hfc_lh * \\mathcal{L}\n",
        "                             + \\mathcal{L}^T * hfc_hl * \\mathcal{H}\n",
        "                             + \\mathcal{H}^T * hfc_hh * \\mathcal{H}\n",
        "        :param LL: the low-frequency component\n",
        "        :param LH: the high-frequency component, hfc_lh\n",
        "        :param HL: the high-frequency component, hfc_hl\n",
        "        :param HH: the high-frequency component, hfc_hh\n",
        "        :return: the original 2D data\n",
        "        \"\"\"\n",
        "        assert len(LL.size()) == len(LH.size()) == len(\n",
        "            HL.size()) == len(HH.size()) == 4\n",
        "        self.input_height = LL.size()[-2] + HH.size()[-2]\n",
        "        self.input_width = LL.size()[-1] + HH.size()[-1]\n",
        "        self.get_matrix()\n",
        "        return IDWTFunction_2D.apply(LL, LH, HL, HH, self.matrix_low_0, self.matrix_low_1, self.matrix_high_0, self.matrix_high_1)\n",
        "\n",
        "\n",
        "class DWT_3D(Module):\n",
        "    \"\"\"\n",
        "    input: the 3D data to be decomposed -- (N, C, D, H, W)\n",
        "    output: lfc -- (N, C, D/2, H/2, W/2)\n",
        "            hfc_llh -- (N, C, D/2, H/2, W/2)\n",
        "            hfc_lhl -- (N, C, D/2, H/2, W/2)\n",
        "            hfc_lhh -- (N, C, D/2, H/2, W/2)\n",
        "            hfc_hll -- (N, C, D/2, H/2, W/2)\n",
        "            hfc_hlh -- (N, C, D/2, H/2, W/2)\n",
        "            hfc_hhl -- (N, C, D/2, H/2, W/2)\n",
        "            hfc_hhh -- (N, C, D/2, H/2, W/2)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wavename):\n",
        "        \"\"\"\n",
        "        3D discrete wavelet transform (DWT) for 3D data decomposition\n",
        "        :param wavename: pywt.wavelist(); in the paper, 'chx.y' denotes 'biorx.y'.\n",
        "        \"\"\"\n",
        "        super(DWT_3D, self).__init__()\n",
        "        wavelet = pywt.Wavelet(wavename)\n",
        "        self.band_low = wavelet.rec_lo\n",
        "        self.band_high = wavelet.rec_hi\n",
        "        assert len(self.band_low) == len(self.band_high)\n",
        "        self.band_length = len(self.band_low)\n",
        "        assert self.band_length % 2 == 0\n",
        "        self.band_length_half = math.floor(self.band_length / 2)\n",
        "\n",
        "    def get_matrix(self):\n",
        "        \"\"\"\n",
        "        生成变换矩阵\n",
        "        generating the matrices: \\mathcal{L}, \\mathcal{H}\n",
        "        :return: self.matrix_low = \\mathcal{L}, self.matrix_high = \\mathcal{H}\n",
        "        \"\"\"\n",
        "        L1 = np.max((self.input_height, self.input_width))\n",
        "        L = math.floor(L1 / 2)\n",
        "        matrix_h = np.zeros((L, L1 + self.band_length - 2))\n",
        "        matrix_g = np.zeros((L1 - L, L1 + self.band_length - 2))\n",
        "        end = None if self.band_length_half == 1 else (\n",
        "            - self.band_length_half + 1)\n",
        "\n",
        "        index = 0\n",
        "        for i in range(L):\n",
        "            for j in range(self.band_length):\n",
        "                matrix_h[i, index + j] = self.band_low[j]\n",
        "            index += 2\n",
        "        matrix_h_0 = matrix_h[0:(math.floor(\n",
        "            self.input_height / 2)), 0:(self.input_height + self.band_length - 2)]\n",
        "        matrix_h_1 = matrix_h[0:(math.floor(\n",
        "            self.input_width / 2)), 0:(self.input_width + self.band_length - 2)]\n",
        "        matrix_h_2 = matrix_h[0:(math.floor(\n",
        "            self.input_depth / 2)), 0:(self.input_depth + self.band_length - 2)]\n",
        "\n",
        "        index = 0\n",
        "        for i in range(L1 - L):\n",
        "            for j in range(self.band_length):\n",
        "                matrix_g[i, index + j] = self.band_high[j]\n",
        "            index += 2\n",
        "        matrix_g_0 = matrix_g[0:(self.input_height - math.floor(\n",
        "            self.input_height / 2)), 0:(self.input_height + self.band_length - 2)]\n",
        "        matrix_g_1 = matrix_g[0:(self.input_width - math.floor(\n",
        "            self.input_width / 2)), 0:(self.input_width + self.band_length - 2)]\n",
        "        matrix_g_2 = matrix_g[0:(self.input_depth - math.floor(\n",
        "            self.input_depth / 2)), 0:(self.input_depth + self.band_length - 2)]\n",
        "\n",
        "        matrix_h_0 = matrix_h_0[:, (self.band_length_half - 1):end]\n",
        "        matrix_h_1 = matrix_h_1[:, (self.band_length_half - 1):end]\n",
        "        matrix_h_1 = np.transpose(matrix_h_1)\n",
        "        matrix_h_2 = matrix_h_2[:, (self.band_length_half - 1):end]\n",
        "\n",
        "        matrix_g_0 = matrix_g_0[:, (self.band_length_half - 1):end]\n",
        "        matrix_g_1 = matrix_g_1[:, (self.band_length_half - 1):end]\n",
        "        matrix_g_1 = np.transpose(matrix_g_1)\n",
        "        matrix_g_2 = matrix_g_2[:, (self.band_length_half - 1):end]\n",
        "        if torch.cuda.is_available():\n",
        "            self.matrix_low_0 = torch.Tensor(matrix_h_0).cuda()\n",
        "            self.matrix_low_1 = torch.Tensor(matrix_h_1).cuda()\n",
        "            self.matrix_low_2 = torch.Tensor(matrix_h_2).cuda()\n",
        "            self.matrix_high_0 = torch.Tensor(matrix_g_0).cuda()\n",
        "            self.matrix_high_1 = torch.Tensor(matrix_g_1).cuda()\n",
        "            self.matrix_high_2 = torch.Tensor(matrix_g_2).cuda()\n",
        "        else:\n",
        "            self.matrix_low_0 = torch.Tensor(matrix_h_0)\n",
        "            self.matrix_low_1 = torch.Tensor(matrix_h_1)\n",
        "            self.matrix_low_2 = torch.Tensor(matrix_h_2)\n",
        "            self.matrix_high_0 = torch.Tensor(matrix_g_0)\n",
        "            self.matrix_high_1 = torch.Tensor(matrix_g_1)\n",
        "            self.matrix_high_2 = torch.Tensor(matrix_g_2)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        :param input: the 3D data to be decomposed\n",
        "        :return: the eight components of the input data, one low-frequency and seven high-frequency components\n",
        "        \"\"\"\n",
        "        assert len(input.size()) == 5\n",
        "        self.input_depth = input.size()[-3]\n",
        "        self.input_height = input.size()[-2]\n",
        "        self.input_width = input.size()[-1]\n",
        "        self.get_matrix()\n",
        "        return DWTFunction_3D.apply(input, self.matrix_low_0, self.matrix_low_1, self.matrix_low_2,\n",
        "                                    self.matrix_high_0, self.matrix_high_1, self.matrix_high_2)\n",
        "\n",
        "\n",
        "class IDWT_3D(Module):\n",
        "    \"\"\"\n",
        "    input:  lfc -- (N, C, D/2, H/2, W/2)\n",
        "            hfc_llh -- (N, C, D/2, H/2, W/2)\n",
        "            hfc_lhl -- (N, C, D/2, H/2, W/2)\n",
        "            hfc_lhh -- (N, C, D/2, H/2, W/2)\n",
        "            hfc_hll -- (N, C, D/2, H/2, W/2)\n",
        "            hfc_hlh -- (N, C, D/2, H/2, W/2)\n",
        "            hfc_hhl -- (N, C, D/2, H/2, W/2)\n",
        "            hfc_hhh -- (N, C, D/2, H/2, W/2)\n",
        "    output: the original 3D data -- (N, C, D, H, W)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wavename):\n",
        "        \"\"\"\n",
        "        3D inverse DWT (IDWT) for 3D data reconstruction\n",
        "        :param wavename: pywt.wavelist(); in the paper, 'chx.y' denotes 'biorx.y'.\n",
        "        \"\"\"\n",
        "        super(IDWT_3D, self).__init__()\n",
        "        wavelet = pywt.Wavelet(wavename)\n",
        "        self.band_low = wavelet.dec_lo\n",
        "        self.band_high = wavelet.dec_hi\n",
        "        self.band_low.reverse()\n",
        "        self.band_high.reverse()\n",
        "        assert len(self.band_low) == len(self.band_high)\n",
        "        self.band_length = len(self.band_low)\n",
        "        assert self.band_length % 2 == 0\n",
        "        self.band_length_half = math.floor(self.band_length / 2)\n",
        "\n",
        "    def get_matrix(self):\n",
        "        \"\"\"\n",
        "        生成变换矩阵\n",
        "        generating the matrices: \\mathcal{L}, \\mathcal{H}\n",
        "        :return: self.matrix_low = \\mathcal{L}, self.matrix_high = \\mathcal{H}\n",
        "        \"\"\"\n",
        "        L1 = np.max((self.input_height, self.input_width))\n",
        "        L = math.floor(L1 / 2)\n",
        "        matrix_h = np.zeros((L, L1 + self.band_length - 2))\n",
        "        matrix_g = np.zeros((L1 - L, L1 + self.band_length - 2))\n",
        "        end = None if self.band_length_half == 1 else (\n",
        "            - self.band_length_half + 1)\n",
        "\n",
        "        index = 0\n",
        "        for i in range(L):\n",
        "            for j in range(self.band_length):\n",
        "                matrix_h[i, index + j] = self.band_low[j]\n",
        "            index += 2\n",
        "        matrix_h_0 = matrix_h[0:(math.floor(\n",
        "            self.input_height / 2)), 0:(self.input_height + self.band_length - 2)]\n",
        "        matrix_h_1 = matrix_h[0:(math.floor(\n",
        "            self.input_width / 2)), 0:(self.input_width + self.band_length - 2)]\n",
        "        matrix_h_2 = matrix_h[0:(math.floor(\n",
        "            self.input_depth / 2)), 0:(self.input_depth + self.band_length - 2)]\n",
        "\n",
        "        index = 0\n",
        "        for i in range(L1 - L):\n",
        "            for j in range(self.band_length):\n",
        "                matrix_g[i, index + j] = self.band_high[j]\n",
        "            index += 2\n",
        "        matrix_g_0 = matrix_g[0:(self.input_height - math.floor(\n",
        "            self.input_height / 2)), 0:(self.input_height + self.band_length - 2)]\n",
        "        matrix_g_1 = matrix_g[0:(self.input_width - math.floor(\n",
        "            self.input_width / 2)), 0:(self.input_width + self.band_length - 2)]\n",
        "        matrix_g_2 = matrix_g[0:(self.input_depth - math.floor(\n",
        "            self.input_depth / 2)), 0:(self.input_depth + self.band_length - 2)]\n",
        "\n",
        "        matrix_h_0 = matrix_h_0[:, (self.band_length_half - 1):end]\n",
        "        matrix_h_1 = matrix_h_1[:, (self.band_length_half - 1):end]\n",
        "        matrix_h_1 = np.transpose(matrix_h_1)\n",
        "        matrix_h_2 = matrix_h_2[:, (self.band_length_half - 1):end]\n",
        "\n",
        "        matrix_g_0 = matrix_g_0[:, (self.band_length_half - 1):end]\n",
        "        matrix_g_1 = matrix_g_1[:, (self.band_length_half - 1):end]\n",
        "        matrix_g_1 = np.transpose(matrix_g_1)\n",
        "        matrix_g_2 = matrix_g_2[:, (self.band_length_half - 1):end]\n",
        "        if torch.cuda.is_available():\n",
        "            self.matrix_low_0 = torch.Tensor(matrix_h_0).cuda()\n",
        "            self.matrix_low_1 = torch.Tensor(matrix_h_1).cuda()\n",
        "            self.matrix_low_2 = torch.Tensor(matrix_h_2).cuda()\n",
        "            self.matrix_high_0 = torch.Tensor(matrix_g_0).cuda()\n",
        "            self.matrix_high_1 = torch.Tensor(matrix_g_1).cuda()\n",
        "            self.matrix_high_2 = torch.Tensor(matrix_g_2).cuda()\n",
        "        else:\n",
        "            self.matrix_low_0 = torch.Tensor(matrix_h_0)\n",
        "            self.matrix_low_1 = torch.Tensor(matrix_h_1)\n",
        "            self.matrix_low_2 = torch.Tensor(matrix_h_2)\n",
        "            self.matrix_high_0 = torch.Tensor(matrix_g_0)\n",
        "            self.matrix_high_1 = torch.Tensor(matrix_g_1)\n",
        "            self.matrix_high_2 = torch.Tensor(matrix_g_2)\n",
        "\n",
        "    def forward(self, LLL, LLH, LHL, LHH, HLL, HLH, HHL, HHH):\n",
        "        \"\"\"\n",
        "        :param LLL: the low-frequency component, lfc\n",
        "        :param LLH: the high-frequency componetn, hfc_llh\n",
        "        :param LHL: the high-frequency componetn, hfc_lhl\n",
        "        :param LHH: the high-frequency componetn, hfc_lhh\n",
        "        :param HLL: the high-frequency componetn, hfc_hll\n",
        "        :param HLH: the high-frequency componetn, hfc_hlh\n",
        "        :param HHL: the high-frequency componetn, hfc_hhl\n",
        "        :param HHH: the high-frequency componetn, hfc_hhh\n",
        "        :return: the original 3D input data\n",
        "        \"\"\"\n",
        "        assert len(LLL.size()) == len(LLH.size()) == len(\n",
        "            LHL.size()) == len(LHH.size()) == 5\n",
        "        assert len(HLL.size()) == len(HLH.size()) == len(\n",
        "            HHL.size()) == len(HHH.size()) == 5\n",
        "        self.input_depth = LLL.size()[-3] + HHH.size()[-3]\n",
        "        self.input_height = LLL.size()[-2] + HHH.size()[-2]\n",
        "        self.input_width = LLL.size()[-1] + HHH.size()[-1]\n",
        "        self.get_matrix()\n",
        "        return IDWTFunction_3D.apply(LLL, LLH, LHL, LHH, HLL, HLH, HHL, HHH,\n",
        "                                     self.matrix_low_0, self.matrix_low_1, self.matrix_low_2,\n",
        "                                     self.matrix_high_0, self.matrix_high_1, self.matrix_high_2)\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     dwt = DWT_2D(\"haar\")\n",
        "#     iwt = IDWT_2D(\"haar\")\n",
        "#     x = torch.randn(3, 3, 24, 24).cuda()\n",
        "#     xll = x\n",
        "#     wavelet_list = []\n",
        "#     for i in range(3):\n",
        "#         xll, xlh, xhl, xhh = dwt(xll)\n",
        "#         wavelet_list.append([xll, xlh, xhl, xhh])\n",
        "\n",
        "#     # xll = wavelet_list[-1] * torch.randn(xll.shape)\n",
        "#     for i in range(2)[::-1]:\n",
        "#         xll, xlh, xhl, xhh = wavelet_list[i]\n",
        "#         xll = iwt(xll, xlh, xhl, xhh)\n",
        "#         print(xll.shape)\n",
        "\n",
        "#     print(torch.sum(x - xll))\n",
        "#     print(torch.sum(x - iwt(*wavelet_list[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPvVSY7546O4"
      },
      "source": [
        "## DWT & IDWT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDYyYih445N9"
      },
      "outputs": [],
      "source": [
        "class DWT(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.dwt = DWT_2D(\"haar\")\n",
        "    def forward(self, x):\n",
        "      xll, xlh, xhl, xhh = self.dwt(x)\n",
        "      xh = torch.cat([xlh, xhl, xhh], 1)\n",
        "      return xll, xh\n",
        "\n",
        "class IDWT(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.idwt = IDWT_2D(\"haar\")\n",
        "    def forward(self, xll, high_freq):\n",
        "      xlh, xhl, xhh = high_freq.chunk(3, dim = 1)\n",
        "      x = self.idwt(xll, xlh, xhl, xhh)\n",
        "      return x\n",
        "\n",
        "class DWT_transform(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.dwt = DWT()\n",
        "        self.conv1x1_low = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
        "        self.conv1x1_high = nn.Conv2d(in_channels*3, out_channels*3, kernel_size=1, padding=0)\n",
        "        # self.conv1x1_mix = nn.Conv2d(out_channels*2, out_channels, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        b, c, h, w = x.shape\n",
        "        mod1 = h % 2\n",
        "        mod2 = w % 2\n",
        "        if (mod1):\n",
        "            # print(\"padding height\")\n",
        "            x = F.pad(x, (0, 0, 0, 1), \"replicate\")\n",
        "        if (mod2):\n",
        "            # print(\"padding width\")\n",
        "            x = F.pad(x, (0, 1, 0, 0), \"replicate\")\n",
        "\n",
        "        dwt_low_frequency, dwt_high_frequency = self.dwt(x)\n",
        "        dwt_low_frequency = self.conv1x1_low(dwt_low_frequency)\n",
        "        dwt_high_frequency = self.conv1x1_high(dwt_high_frequency)\n",
        "        # x = torch.cat([dwt_low_frequency, dwt_high_frequency], 1)\n",
        "        # x = self.conv1x1_mix(x)\n",
        "\n",
        "        return dwt_low_frequency, dwt_high_frequency, [mod1, mod2]\n",
        "\n",
        "class IDWT_transform(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1x1_low = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
        "        self.conv1x1_high = nn.Conv2d(in_channels*3, out_channels*3, kernel_size=1, padding=0)\n",
        "        self.idwt = IDWT()\n",
        "\n",
        "    def forward(self, x, high_freq):\n",
        "        x = self.conv1x1_low(x)\n",
        "        high_freq = self.conv1x1_high(high_freq)\n",
        "        x = self.idwt(x, high_freq)\n",
        "\n",
        "        return x\n",
        "\n",
        "class DWT_IDWT_transform(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      # self.conv1x1_low = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
        "      # self.conv1x1_high = nn.Conv2d(in_channels*3, out_channels*3, kernel_size=1, padding=0)\n",
        "      self.dwt = DWT()\n",
        "      self.idwt = IDWT()\n",
        "  def forward(self, x):\n",
        "      b, c, h, w = x.shape\n",
        "      mod1 = h % 2\n",
        "      mod2 = w % 2\n",
        "      if (mod1):\n",
        "          # print(\"padding height\")\n",
        "          x = F.pad(x, (0, 0, 0, 1), \"replicate\")\n",
        "      if (mod2):\n",
        "          # print(\"padding width\")\n",
        "          x = F.pad(x, (0, 1, 0, 0), \"replicate\")\n",
        "\n",
        "      xll, xh = self.dwt(x)\n",
        "      x = self.idwt(xll, xh)\n",
        "\n",
        "      if (mod1):\n",
        "        x = x[:, :, :-1, :]\n",
        "      if (mod2):\n",
        "        x = x[:, :, :, :-1]\n",
        "\n",
        "      return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrfC9Co5vJ5G"
      },
      "source": [
        "## New DWT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KtLxxzQvMQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "036e01b6-5fdd-46f6-f190-b8617db39206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_wavelets\n",
            "  Downloading pytorch_wavelets-1.3.0-py3-none-any.whl (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_wavelets) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pytorch_wavelets) (1.16.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pytorch_wavelets) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_wavelets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_wavelets) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_wavelets) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_wavelets) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_wavelets) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_wavelets) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pytorch_wavelets) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pytorch_wavelets) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pytorch_wavelets) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pytorch_wavelets) (1.3.0)\n",
            "Installing collected packages: pytorch_wavelets\n",
            "Successfully installed pytorch_wavelets-1.3.0\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  from pytorch_wavelets import DWTForward, DWTInverse\n",
        "except:\n",
        "  !pip install pytorch_wavelets\n",
        "  from pytorch_wavelets import DWTForward, DWTInverse\n",
        "\n",
        "class DWT_transform_2(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.dwt = DWTForward(J=1, wave='haar', mode='reflect')\n",
        "        self.conv1x1_low = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
        "        self.conv1x1_high = nn.Conv2d(in_channels*3, out_channels*3, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        dwt_low_frequency, dwt_high_frequency = self.dwt(x)\n",
        "        dwt_low_frequency = self.conv1x1_low(dwt_low_frequency)\n",
        "        dwt_high_frequency[0] = rearrange(dwt_high_frequency[0], 'b c freq h w -> b (c freq) h w', freq=3)\n",
        "        dwt_high_frequency[0] = self.conv1x1_high(dwt_high_frequency[0])\n",
        "\n",
        "        return dwt_low_frequency, dwt_high_frequency\n",
        "\n",
        "class IDWT_transform_2(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1x1_low = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
        "        self.conv1x1_high = nn.Conv2d(in_channels*3, out_channels*3, kernel_size=1, padding=0)\n",
        "        self.idwt = DWTInverse(wave='haar', mode='reflect')\n",
        "\n",
        "    def forward(self, x, high_freq):\n",
        "        x = self.conv1x1_low(x)\n",
        "        high_freq[0] = self.conv1x1_high(high_freq[0])\n",
        "        high_freq[0] = rearrange(high_freq[0], 'b (c freq) h w -> b c freq h w', freq=3)\n",
        "        x = self.idwt((x, high_freq))\n",
        "\n",
        "        return x\n",
        "\n",
        "class DWT_IDWT_transform_2(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      self.dwt = DWTForward(J=1, wave='haar', mode='reflect')\n",
        "      self.idwt = DWTInverse(wave='haar', mode='reflect')\n",
        "  def forward(self, x):\n",
        "      xll, xh = self.dwt(x)\n",
        "      xh[0] = rearrange(xh[0], 'b c freq h w -> b (c freq) h w', freq=3)\n",
        "      xh[0] = rearrange(xh[0], 'b (c freq) h w -> b c freq h w', freq=3)\n",
        "      x = self.idwt((xll, xh))\n",
        "\n",
        "      return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT7XdDldATtJ"
      },
      "source": [
        "# Basic Residual Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3JR3sgwAYLE"
      },
      "outputs": [],
      "source": [
        "class BasicResBlock(nn.Module):\n",
        "\tdef __init__(self, channel_num):\n",
        "\t\tsuper(BasicResBlock, self).__init__()\n",
        "\n",
        "\t\t#the input and output channel number is channel_num\n",
        "\t\tself.conv_block1 = nn.Sequential(\n",
        "\t\t\tnn.Conv2d(channel_num, channel_num, 3, padding=1),\n",
        "\t\t\tnn.BatchNorm2d(channel_num),\n",
        "\t\t\tnn.ReLU(inplace=True),\n",
        "\t\t)\n",
        "\t\tself.conv_block2 = nn.Sequential(\n",
        "\t\t\tnn.Conv2d(channel_num, channel_num, 3, padding=1),\n",
        "\t\t\tnn.BatchNorm2d(channel_num),\n",
        "\t\t)\n",
        "\t\t# self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\n",
        "\t\tresidual = x\n",
        "\t\tx = self.conv_block1(x)\n",
        "\t\tx = self.conv_block2(x)\n",
        "\t\tx = x + residual\n",
        "\t\t# out = self.relu(x)\n",
        "\t\treturn x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db8pIBHce7Tq"
      },
      "source": [
        "# Selective Res Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNcxwbg0e-8k"
      },
      "outputs": [],
      "source": [
        "def _make_conv_layer(in_channels, out_channels, stride=1, dilation=1, norm_type='bn'):\n",
        "    conv_layer = [\n",
        "        nn.ReflectionPad2d(dilation),\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=0, dilation=dilation)\n",
        "    ]\n",
        "\n",
        "    if norm_type == 'bn':\n",
        "        conv_layer.append(nn.BatchNorm2d(out_channels))\n",
        "    elif norm_type == 'in':\n",
        "        conv_layer.append(nn.InstanceNorm2d(out_channels))\n",
        "\n",
        "    return nn.Sequential(*conv_layer)\n",
        "\n",
        "class selective_res_block(nn.Module):\n",
        "    def __init__(self, channels, stride=1, activation=nn.ReLU(inplace=True), norm_type='bn'):\n",
        "        super(selective_res_block, self).__init__()\n",
        "        self.conv1 = _make_conv_layer(channels, channels, stride=stride, norm_type=norm_type)\n",
        "        self.conv2 = _make_conv_layer(channels, channels, stride=stride, norm_type=norm_type)\n",
        "        self.act = activation\n",
        "        self.a = nn.Parameter(data=torch.ones(1))\n",
        "        self.b = nn.Parameter(data=torch.ones(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.act(self.conv1(x))\n",
        "        out = self.conv2(out)\n",
        "        out = self.act(out.mul(self.a) + identity.mul(self.b)) # weighted sum\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDgYd9Dygr8q"
      },
      "source": [
        "# RESTBlock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fB5mRpqlgv1E"
      },
      "outputs": [],
      "source": [
        "import numbers\n",
        "\n",
        "##########################################################################\n",
        "## Layer Norm\n",
        "\n",
        "def to_3d(x):\n",
        "    return rearrange(x, 'b c h w -> b (h w) c')\n",
        "    # flatten = nn.Flatten(2,3)\n",
        "    # return flatten(x).permute(0,2,1)\n",
        "\n",
        "def to_4d(x,h,w):\n",
        "    return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)\n",
        "    # unflatten = nn.Unflatten(1,(h,w))\n",
        "    # return unflatten(x).permute(0,3,1,2)\n",
        "\n",
        "class BiasFree_LayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape):\n",
        "        super(BiasFree_LayerNorm, self).__init__()\n",
        "        if isinstance(normalized_shape, numbers.Integral):\n",
        "            normalized_shape = (normalized_shape,)\n",
        "        normalized_shape = torch.Size(normalized_shape)\n",
        "\n",
        "        assert len(normalized_shape) == 1\n",
        "\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
        "        return x / torch.sqrt(sigma+1e-5) * self.weight\n",
        "\n",
        "class WithBias_LayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape):\n",
        "        super(WithBias_LayerNorm, self).__init__()\n",
        "        if isinstance(normalized_shape, numbers.Integral):\n",
        "            normalized_shape = (normalized_shape,)\n",
        "        normalized_shape = torch.Size(normalized_shape)\n",
        "\n",
        "        assert len(normalized_shape) == 1\n",
        "\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu = x.mean(-1, keepdim=True)\n",
        "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
        "        return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias\n",
        "\n",
        "\n",
        "class RESTLayerNorm(nn.Module):\n",
        "    def __init__(self, dim, LayerNorm_type):\n",
        "        super(RESTLayerNorm, self).__init__()\n",
        "        if LayerNorm_type =='BiasFree':\n",
        "            self.body = BiasFree_LayerNorm(dim)\n",
        "        else:\n",
        "            self.body = WithBias_LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, w = x.shape[-2:]\n",
        "        return to_4d(self.body(to_3d(x)), h, w)\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "## Gated-Dconv Feed-Forward Network (GDFN)\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
        "        super(FeedForward, self).__init__()\n",
        "\n",
        "        hidden_features = int(dim*ffn_expansion_factor)\n",
        "\n",
        "        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n",
        "\n",
        "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n",
        "\n",
        "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.project_in(x)\n",
        "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
        "        x = F.gelu(x1) * x2\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "## Multi-DConv Head Transposed Self-Attention (MDTA)\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads, bias):\n",
        "        super(Attention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
        "\n",
        "        self.qkv = nn.Conv2d(dim, dim*3, kernel_size=1, bias=bias)\n",
        "        self.qkv_dwconv = nn.Conv2d(dim*3, dim*3, kernel_size=3, stride=1, padding=1, groups=dim*3, bias=bias)\n",
        "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        b,c,h,w = x.shape\n",
        "\n",
        "        qkv = self.qkv_dwconv(self.qkv(x))\n",
        "        q,k,v = qkv.chunk(3, dim=1)\n",
        "\n",
        "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
        "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
        "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
        "\n",
        "        q = torch.nn.functional.normalize(q, dim=-1)\n",
        "        k = torch.nn.functional.normalize(k, dim=-1)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        out = (attn @ v)\n",
        "\n",
        "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
        "\n",
        "        out = self.project_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, ffn_expansion_factor = 2.66, bias = False, LayerNorm_type = 'WithBias'):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.norm1 = RESTLayerNorm(dim, LayerNorm_type)\n",
        "        self.attn = Attention(dim, num_heads, bias)\n",
        "        self.norm2 = RESTLayerNorm(dim, LayerNorm_type)\n",
        "        self.ffn = FeedForward(dim, ffn_expansion_factor, bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.ffn(self.norm2(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8W3NwOJ7EIL"
      },
      "source": [
        "# Bottleneck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZijlL427Gjh"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * 4)\n",
        "        self.bn3 = norm_layer(planes * 4)\n",
        "        self.relu = nn.LeakyReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out = out + identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVS40dtjEcb7"
      },
      "source": [
        "# Xception (UNet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMhX1iV-EfVK"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Creates an Xception Model as defined in:\n",
        "\n",
        "Francois Chollet\n",
        "Xception: Deep Learning with Depthwise Separable Convolutions\n",
        "https://arxiv.org/pdf/1610.02357.pdf\n",
        "\n",
        "This weights ported from the Keras implementation. Achieves the following performance on the validation set:\n",
        "\n",
        "Loss:0.9173 Prec@1:78.892 Prec@5:94.292\n",
        "\n",
        "REMEMBER to set your image size to 3x299x299 for both test and validation\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                  std=[0.5, 0.5, 0.5])\n",
        "\n",
        "The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n",
        "\"\"\"\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# import torch.utils.model_zoo as model_zoo\n",
        "from torch.nn import init\n",
        "import torch\n",
        "\n",
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n",
        "        super(SeparableConv2d,self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n",
        "        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self,in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "        if out_filters != in_filters or strides!=1:\n",
        "            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n",
        "            self.skipbn = nn.BatchNorm2d(out_filters)\n",
        "        else:\n",
        "            self.skip=None\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        rep=[]\n",
        "\n",
        "        filters=in_filters\n",
        "        if grow_first:\n",
        "            rep.append(self.relu)\n",
        "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
        "            rep.append(nn.BatchNorm2d(out_filters))\n",
        "            filters = out_filters\n",
        "\n",
        "        for i in range(reps-1):\n",
        "            rep.append(self.relu)\n",
        "            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n",
        "            rep.append(nn.BatchNorm2d(filters))\n",
        "\n",
        "        if not grow_first:\n",
        "            rep.append(self.relu)\n",
        "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
        "            rep.append(nn.BatchNorm2d(out_filters))\n",
        "\n",
        "        if not start_with_relu:\n",
        "            rep = rep[1:]\n",
        "        else:\n",
        "            rep[0] = nn.ReLU(inplace=False)\n",
        "\n",
        "        if strides != 1:\n",
        "            rep.append(nn.MaxPool2d(3,strides,1))\n",
        "        self.rep = nn.Sequential(*rep)\n",
        "\n",
        "    def forward(self,inp):\n",
        "        x = self.rep(inp)\n",
        "\n",
        "        if self.skip is not None:\n",
        "            skip = self.skip(inp)\n",
        "            skip = self.skipbn(skip)\n",
        "        else:\n",
        "            skip = inp\n",
        "\n",
        "        x+=skip\n",
        "        return x\n",
        "\n",
        "class PALayer(nn.Module):\n",
        "    def __init__(self, channel):\n",
        "        super(PALayer, self).__init__()\n",
        "        self.pa = nn.Sequential(\n",
        "            nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channel // 8, 1, 1, padding=0, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        y = self.pa(x)\n",
        "        return x * y\n",
        "\n",
        "class CALayer(nn.Module):\n",
        "    def __init__(self, channel):\n",
        "        super(CALayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.ca = nn.Sequential(\n",
        "            nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channel // 8, channel, 1, padding=0, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        y = self.avg_pool(x)\n",
        "        y = self.ca(y)\n",
        "        return x * y\n",
        "\n",
        "class CP_Attention_block(nn.Module):\n",
        "    def __init__(self, conv, dim, kernel_size):\n",
        "        super(CP_Attention_block, self).__init__()\n",
        "        self.conv1 = conv(dim, dim, kernel_size, bias=True)\n",
        "        self.act1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv(dim, dim, kernel_size, bias=True)\n",
        "        self.calayer = CALayer(dim)\n",
        "        self.palayer = PALayer(dim)\n",
        "    def forward(self, x):\n",
        "        res = self.act1(self.conv1(x))\n",
        "        res = res + x\n",
        "        res = self.conv2(res)\n",
        "        res = self.calayer(res)\n",
        "        res = self.palayer(res)\n",
        "        res += x\n",
        "        return res\n",
        "\n",
        "def default_conv(in_channels, out_channels, kernel_size, bias=True):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size // 2), bias=bias)\n",
        "\n",
        "class Xception(nn.Module):\n",
        "    \"\"\"\n",
        "    Xception optimized for the ImageNet dataset, as specified in\n",
        "    https://arxiv.org/pdf/1610.02357.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\" Constructor\n",
        "        Args:\n",
        "            num_classes: number of classes\n",
        "        \"\"\"\n",
        "        super(Xception, self).__init__()\n",
        "\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3,2, 0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        #do relu here\n",
        "\n",
        "        self.block1=Block(64,128,2,2,start_with_relu=False,grow_first=True)\n",
        "        self.block2=Block(128,256,2,2,start_with_relu=True,grow_first=True)\n",
        "        self.block3=Block(256,728,2,2,start_with_relu=True,grow_first=True)\n",
        "\n",
        "        self.block4=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block5=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block6=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block7=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "\n",
        "        self.block8=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block9=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block10=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block11=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "\n",
        "        self.block12=Block(728,1024,2,2,start_with_relu=True,grow_first=False)\n",
        "\n",
        "        self.conv3 = SeparableConv2d(1024,1536,3,1,1)\n",
        "        self.bn3 = nn.BatchNorm2d(1536)\n",
        "\n",
        "        #do relu here\n",
        "        self.conv4 = SeparableConv2d(1536,2048,3,1,1)\n",
        "        self.bn4 = nn.BatchNorm2d(2048)\n",
        "\n",
        "        # self.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "        #------- init weights --------\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "        #-----------------------------\n",
        "\n",
        "\n",
        "        self.up_block= nn.PixelShuffle(2)\n",
        "        self.attention0 = CP_Attention_block(default_conv, 512, 3)\n",
        "        self.conv_process0 = nn.Conv2d(512, 1024, kernel_size=3,padding=1)\n",
        "\n",
        "        #upsample here\n",
        "        self.attention1 = CP_Attention_block(default_conv, 256, 3)\n",
        "        self.conv_process1 = nn.Conv2d(256, 512, kernel_size=3,padding=1)\n",
        "\n",
        "        #upsample here\n",
        "        self.attention2 = CP_Attention_block(default_conv, 128, 3)\n",
        "        self.conv_process2 = nn.Conv2d(128, 256, kernel_size=3,padding=1)\n",
        "\n",
        "        #upsample here\n",
        "        self.attention3 = CP_Attention_block(default_conv, 64, 3)\n",
        "        self.conv_process3 = nn.Conv2d(64, 128, kernel_size=3,padding=1)\n",
        "\n",
        "        #upsample here\n",
        "        self.attention4 = CP_Attention_block(default_conv, 32, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        # print(\"1: \", x.size())\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        # print(\"2: \", x.size())\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.block5(x)\n",
        "        x = self.block6(x)\n",
        "        x = self.block7(x)\n",
        "        x = self.block8(x)\n",
        "        x = self.block9(x)\n",
        "        x = self.block10(x)\n",
        "        x = self.block11(x)\n",
        "        x = self.block12(x)\n",
        "        # print(\"3: \", x.size())\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "        # print(\"4: \", x.size())\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.relu(x) # channels = 2048\n",
        "        # print(\"5: \", x.size())\n",
        "\n",
        "        # x = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        # x = x.view(x.size(0), -1)\n",
        "        # x = self.fc(x)\n",
        "\n",
        "\n",
        "        x = self.up_block(x) # channels = 512\n",
        "        x = self.attention0(x)\n",
        "        x = self.conv_process0(x) # channels = 1024\n",
        "        # print(\"6: \", x.size())\n",
        "\n",
        "        x = self.up_block(x) # channels = 256\n",
        "        x = self.attention1(x)\n",
        "        x = self.conv_process1(x) # channels = 512\n",
        "        # print(\"7: \", x.size())\n",
        "\n",
        "        x = self.up_block(x) # channels = 128\n",
        "        x = self.attention2(x)\n",
        "        x = self.conv_process2(x) # channels = 256\n",
        "        # print(\"8: \", x.size())\n",
        "\n",
        "        x = self.up_block(x) # channels = 64\n",
        "        x = self.attention3(x)\n",
        "        x = self.conv_process3(x) # channels = 128\n",
        "        # print(\"9: \", x.size())\n",
        "\n",
        "        x = self.up_block(x) # channels = 32\n",
        "        x = self.attention4(x)\n",
        "        # print(\"10: \", x.size())\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# def xception(pretrained=False,**kwargs):\n",
        "#     \"\"\"\n",
        "#     Construct Xception.\n",
        "#     \"\"\"\n",
        "\n",
        "#     model = Xception(**kwargs)\n",
        "#     if pretrained:\n",
        "#         model.load_state_dict(model_zoo.load_url(model_urls['xception']), strict = False)\n",
        "#     return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dabbMp9rYnwm"
      },
      "source": [
        "# Model (Main UNet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fADmCM-93u_l"
      },
      "outputs": [],
      "source": [
        "from torch.functional import Tensor\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from functools import partial\n",
        "import math\n",
        "import warnings\n",
        "import torch.nn.functional as f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCahn0Dx7juC"
      },
      "source": [
        "## Residual & Transformer Sequence Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJwZhX1ldnSx"
      },
      "outputs": [],
      "source": [
        "class BRB_Transformer(nn.Module):\n",
        "  def __init__(self, channels, heads):\n",
        "    super(BRB_Transformer, self).__init__()\n",
        "    self.ResBlock = BasicResBlock(channel_num = channels)\n",
        "    self.TransformerBlock = TransformerBlock(dim = channels, num_heads = heads)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.ResBlock(x)\n",
        "    x = self.TransformerBlock(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RjStUdoC3Cy"
      },
      "source": [
        "## Refinement Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xApamuUdC5d6"
      },
      "outputs": [],
      "source": [
        "class WAB(nn.Module):\n",
        "    def __init__(self,n_feats,expand=4):\n",
        "        super(WAB, self).__init__()\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Conv2d(n_feats, n_feats * expand,3,1,1, bias=True),\n",
        "            nn.BatchNorm2d(n_feats * expand),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(n_feats* expand, n_feats , 3, 1, 1, bias=True),\n",
        "            nn.BatchNorm2d(n_feats)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.body(x).mul(0.2)+x\n",
        "        return res\n",
        "\n",
        "\n",
        "class invPixelShuffle(nn.Module):\n",
        "    def __init__(self, ratio=2):\n",
        "        super(invPixelShuffle, self).__init__()\n",
        "        self.ratio = ratio\n",
        "\n",
        "    def forward(self, tensor):\n",
        "        ratio = self.ratio\n",
        "        b, ch, y, x = tensor.shape\n",
        "        assert x % ratio == 0 and y % ratio == 0, 'x, y, ratio : {}, {}, {}'.format(x, y, ratio)\n",
        "        return tensor.view(b, ch, y // ratio, ratio, x // ratio, ratio).permute(0, 1, 3, 5, 2, 4).contiguous().view(b,-1,y // ratio,x // ratio)\n",
        "\n",
        "class RefinementBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(RefinementBlock, self).__init__()\n",
        "        self.refinement=nn.Sequential(\n",
        "                  nn.Conv2d(channels,16,3,1,1, bias=True),\n",
        "                  nn.BatchNorm2d(16),\n",
        "                  invPixelShuffle(2),\n",
        "                  nn.Conv2d(64,16,3,1,1, bias=True),\n",
        "                  nn.BatchNorm2d(16),\n",
        "                  nn.Sequential(*[WAB(16) for _ in range(3)]),\n",
        "                  nn.Conv2d(16, 64, 3, 1, 1, bias=True),\n",
        "                  nn.PixelShuffle(2),\n",
        "                  nn.BatchNorm2d(16),\n",
        "                  nn.Conv2d(16, channels, 3, 1, 1, bias=True)\n",
        "              )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.refinement(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxFw0ett7skh"
      },
      "source": [
        "## Encoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4I5YFvE38-Q"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, levels = 2, heads = 4):\n",
        "    super(EncoderBlock, self).__init__()\n",
        "\n",
        "    self.ResBlock = BasicResBlock(channel_num = in_channels)\n",
        "    self.DWT = DWT_transform(in_channels = in_channels, out_channels = out_channels)\n",
        "    self.BRBTransformer = nn.Sequential(*[BRB_Transformer(channels = out_channels, heads = heads) for i in range(levels)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.ResBlock(x)\n",
        "    x, high_freq, pad = self.DWT(x)\n",
        "    x = self.BRBTransformer(x)\n",
        "\n",
        "    return x, high_freq, pad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX6y0TcI7u1N"
      },
      "source": [
        "## Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI-ZOZJ342lU"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, levels = 2, heads = 4):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "\n",
        "    self.BRBTransformer_lh = nn.Sequential(*[BRB_Transformer(channels = in_channels, heads = heads) for i in range(levels)])\n",
        "    self.BRBTransformer_hl = nn.Sequential(*[BRB_Transformer(channels = in_channels, heads = heads) for i in range(levels)])\n",
        "    self.BRBTransformer_hh = nn.Sequential(*[BRB_Transformer(channels = in_channels, heads = heads) for i in range(levels)])\n",
        "    # self.IDWT = IDWT_transform(in_channels, out_channels)\n",
        "    self.Deconv = nn.Sequential(\n",
        "          nn.ConvTranspose2d(in_channels * 4, in_channels * 2, kernel_size=2, stride=2, padding=0),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.Conv2d(in_channels * 2, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "          nn.ReLU(inplace=True))\n",
        "    self.Refinement = RefinementBlock(channels = out_channels)\n",
        "    # self.ResBlock = BasicResBlock(channel_num = out_channels)\n",
        "\n",
        "  def forward(self, x, high_freq, pad):\n",
        "    xlh, xhl, xhh = high_freq.chunk(3, dim = 1)\n",
        "    xlh = self.BRBTransformer_lh(xlh)\n",
        "    xhl = self.BRBTransformer_hl(xhl)\n",
        "    xhh = self.BRBTransformer_hh(xhh)\n",
        "    x = torch.cat([x, xlh, xhl, xhh], 1)\n",
        "    x = self.Deconv(x)\n",
        "    x = self.Refinement(x)\n",
        "    if (pad[0]):\n",
        "      x = x[:, :, :-1, :]\n",
        "    if (pad[1]):\n",
        "      x = x[:, :, :, :-1]\n",
        "    # x = self.ResBlock(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Qbmq-U7wj0"
      },
      "source": [
        "## UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg1HfwW1YnQc"
      },
      "outputs": [],
      "source": [
        "class CANT_HAZE(nn.Module):\n",
        "    def __init__(self, channels = 32, levels = 2, heads = 4):\n",
        "      super(CANT_HAZE, self).__init__()\n",
        "\n",
        "      self.EncoderL1 = EncoderBlock(in_channels = 3           , out_channels = channels    , levels = levels, heads = heads)\n",
        "      self.EncoderL2 = EncoderBlock(in_channels = channels    , out_channels = channels * 2, levels = levels, heads = heads)\n",
        "      self.EncoderL3 = EncoderBlock(in_channels = channels * 2, out_channels = channels * 4, levels = levels, heads = heads)\n",
        "      self.EncoderL4 = EncoderBlock(in_channels = channels * 4, out_channels = channels * 8, levels = levels, heads = heads)\n",
        "\n",
        "      self.Bottleneck = Bottleneck(inplanes = channels * 8, planes= int((channels * 8)/4))\n",
        "\n",
        "      self.DecoderL1 = DecoderBlock(in_channels = channels * 8, out_channels = channels * 4, levels = levels, heads = heads)\n",
        "      self.DecoderL2 = DecoderBlock(in_channels = channels * 4, out_channels = channels * 2, levels = levels, heads = heads)\n",
        "      self.DecoderL3 = DecoderBlock(in_channels = channels * 2, out_channels = channels    , levels = levels, heads = heads)\n",
        "      self.DecoderL4 = DecoderBlock(in_channels = channels    , out_channels = 3          , levels = levels, heads = heads)\n",
        "\n",
        "      # self.outResBlock = BasicResBlock(channel_num = 3)\n",
        "      # self.out = nn.Conv2d(3, 3, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "      #-------Encoder L1-------#\n",
        "      e1, high_freq1, pad1 = self.EncoderL1(input)\n",
        "\n",
        "      #-------Encoder L2-------#\n",
        "      e2, high_freq2, pad2 = self.EncoderL2(e1)\n",
        "\n",
        "      #-------Encoder L3-------#\n",
        "      e3, high_freq3, pad3 = self.EncoderL3(e2)\n",
        "\n",
        "      #-------Encoder L4-------#\n",
        "      x, high_freq4, pad4 = self.EncoderL4(e3)\n",
        "\n",
        "      #-------ResNet Bottleneck-------#\n",
        "      x = self.Bottleneck(x)\n",
        "\n",
        "      #-------Decoder L1-------#\n",
        "      x = self.DecoderL1(x, high_freq4, pad4)\n",
        "\n",
        "      #-------Decoder L2-------#\n",
        "      x = self.DecoderL2(x, high_freq3, pad3)\n",
        "\n",
        "      #-------Decoder L3-------#\n",
        "      x = self.DecoderL3(x, high_freq2, pad2)\n",
        "\n",
        "      #-------Decoder L4-------#\n",
        "      # x = self.DecoderL4(x, high_freq1, pad1) + input\n",
        "      x = self.DecoderL4(x, high_freq1, pad1)\n",
        "\n",
        "      #-------Refinement Block-------#\n",
        "      # x = self.outResBlock(x)\n",
        "      # x = self.out(x)\n",
        "\n",
        "      return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9_x92MqJtIL"
      },
      "source": [
        "# Fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLCcU_PJJwZb"
      },
      "outputs": [],
      "source": [
        "class fusion_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(fusion_net, self).__init__()\n",
        "        self.main_branch=CANT_HAZE()\n",
        "        self.knowledge_adaptation_branch=Xception()\n",
        "\n",
        "        self.knowledge_adaptation_branch.load_state_dict(torch.load(\"/content/drive/MyDrive/Graduation Project/CANT_Haze_v10/weights/xception-43020ad28.pth\"),\n",
        "                                                         strict = False)\n",
        "        self.fusion = nn.Sequential(nn.ReflectionPad2d(3), nn.Conv2d(35, 3, kernel_size=7, padding=0), nn.Tanh())\n",
        "    def forward(self, x):\n",
        "        main_branch=self.main_branch(x)\n",
        "\n",
        "        b, c, h, w = x.shape\n",
        "        mod1 = h % 32\n",
        "        mod2 = w % 32\n",
        "        if (mod1):\n",
        "            # print(\"padding height\")\n",
        "            x = F.pad(x, (0, 0, 0, mod1), \"replicate\")\n",
        "        if (mod2):\n",
        "            # print(\"padding width\")\n",
        "            x = F.pad(x, (0, mod2, 0, 0), \"replicate\")\n",
        "\n",
        "        x=self.knowledge_adaptation_branch(x)\n",
        "\n",
        "        if (mod1):\n",
        "          x = x[:, :, :-mod1, :]\n",
        "        if (mod2):\n",
        "          x = x[:, :, :, :-mod2]\n",
        "\n",
        "        x = torch.cat([main_branch, x], 1)\n",
        "        x = self.fusion(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8Qs4ZIDYhVE"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyVbD7-1YjfM",
        "outputId": "8b42da50-57ca-4ea0-fa0e-ee9af6b61455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyEnsembleNet parameters: 59512615\n",
            "Dataset has: 40 images\n",
            "Dataset has: 5 images\n",
            "--- no weight loaded ---\n",
            "We are in epoch: 1\n",
            "AVG PSNR:  11.001591341836113 AVG SSIM:  0.17169991242034094 AVG Loss:  0.5297169621501651\n",
            "We are in epoch: 2\n",
            "AVG PSNR:  12.446638720376152 AVG SSIM:  0.3784032868487494 AVG Loss:  0.4062989034823009\n",
            "We are in epoch: 3\n",
            "AVG PSNR:  13.667108263288226 AVG SSIM:  0.4471408554485866 AVG Loss:  0.3623412123748234\n",
            "We are in epoch: 4\n",
            "AVG PSNR:  12.668119634900775 AVG SSIM:  0.43018369376659393 AVG Loss:  0.3868142089673451\n",
            "We are in epoch: 5\n",
            "AVG PSNR:  14.391375269208636 AVG SSIM:  0.48070192337036133 AVG Loss:  0.3427429028919765\n",
            "-----Testing-----\n",
            "PSNR:  14.28794460296631 SSIM:  0.4824086368083954\n",
            "-----Model Saved-----\n",
            "We are in epoch: 6\n",
            "AVG PSNR:  15.364476203918457 AVG SSIM:  0.5425954737833568 AVG Loss:  0.3006124123930931\n",
            "We are in epoch: 7\n",
            "AVG PSNR:  15.807620184762138 AVG SSIM:  0.573413188968386 AVG Loss:  0.27995134464332033\n",
            "We are in epoch: 8\n",
            "AVG PSNR:  17.059191635676793 AVG SSIM:  0.6095201798847744 AVG Loss:  0.25342040828296114\n",
            "We are in epoch: 9\n",
            "AVG PSNR:  16.77135419845581 AVG SSIM:  0.6222993178027016 AVG Loss:  0.24748755459274566\n",
            "We are in epoch: 10\n",
            "AVG PSNR:  16.953287805829728 AVG SSIM:  0.6317984802382333 AVG Loss:  0.24454130977392197\n",
            "-----Testing-----\n",
            "PSNR:  12.170677375793456 SSIM:  0.5814081788063049\n",
            "-----Model Saved-----\n",
            "We are in epoch: 11\n",
            "AVG PSNR:  15.760370118277413 AVG SSIM:  0.594866503562246 AVG Loss:  0.2718241885304451\n",
            "We are in epoch: 12\n",
            "AVG PSNR:  16.543009076799667 AVG SSIM:  0.6390597224235535 AVG Loss:  0.2409373883690153\n",
            "We are in epoch: 13\n",
            "AVG PSNR:  17.857204096657888 AVG SSIM:  0.6826181922640119 AVG Loss:  0.21263784489461354\n",
            "We are in epoch: 14\n",
            "AVG PSNR:  17.696462903703964 AVG SSIM:  0.6438748836517334 AVG Loss:  0.23210081883839198\n",
            "We are in epoch: 15\n",
            "AVG PSNR:  17.238098008292063 AVG SSIM:  0.6220009326934814 AVG Loss:  0.24966458124773844\n",
            "-----Testing-----\n",
            "PSNR:  19.148122024536132 SSIM:  0.6813275694847107\n",
            "-----Model Saved-----\n",
            "We are in epoch: 16\n",
            "AVG PSNR:  16.753002371106827 AVG SSIM:  0.6620773885931287 AVG Loss:  0.23167687228747777\n",
            "We are in epoch: 17\n",
            "AVG PSNR:  17.071382999420166 AVG SSIM:  0.6270239246743066 AVG Loss:  0.24583720628704345\n",
            "We are in epoch: 18\n",
            "AVG PSNR:  18.84024667739868 AVG SSIM:  0.6607660693781716 AVG Loss:  0.21570427822215216\n",
            "We are in epoch: 19\n",
            "AVG PSNR:  17.771106447492325 AVG SSIM:  0.6508783314909253 AVG Loss:  0.22929154123578752\n",
            "We are in epoch: 20\n",
            "AVG PSNR:  18.20700400216239 AVG SSIM:  0.6574247777462006 AVG Loss:  0.22327773911612375\n",
            "-----Testing-----\n",
            "PSNR:  18.505580520629884 SSIM:  0.6568634510040283\n",
            "-----Model Saved-----\n",
            "We are in epoch: 21\n",
            "AVG PSNR:  16.916085447583878 AVG SSIM:  0.6695024924618858 AVG Loss:  0.22521360857146128\n",
            "We are in epoch: 22\n",
            "AVG PSNR:  17.42179482323783 AVG SSIM:  0.6681175785405296 AVG Loss:  0.221815502004964\n",
            "We are in epoch: 23\n",
            "AVG PSNR:  17.165673868996755 AVG SSIM:  0.656991720199585 AVG Loss:  0.23030107894114085\n",
            "We are in epoch: 24\n",
            "AVG PSNR:  17.808934484209335 AVG SSIM:  0.6716417499950954 AVG Loss:  0.22011555624859674\n",
            "We are in epoch: 25\n",
            "AVG PSNR:  17.8333580834525 AVG SSIM:  0.6540253758430481 AVG Loss:  0.2269024465765272\n",
            "-----Testing-----\n",
            "PSNR:  19.800847244262695 SSIM:  0.6775964856147766\n",
            "-----Model Saved-----\n",
            "We are in epoch: 26\n",
            "AVG PSNR:  18.78585468019758 AVG SSIM:  0.6894072549683707 AVG Loss:  0.20350917748042516\n",
            "We are in epoch: 27\n",
            "AVG PSNR:  18.175183023725236 AVG SSIM:  0.6626876784222466 AVG Loss:  0.22251727219138825\n",
            "We are in epoch: 28\n",
            "AVG PSNR:  17.54560926982335 AVG SSIM:  0.6391329999480929 AVG Loss:  0.2370373500244958\n",
            "We are in epoch: 29\n",
            "AVG PSNR:  19.33976309640067 AVG SSIM:  0.6824795476027897 AVG Loss:  0.2032691655414445\n",
            "We are in epoch: 30\n",
            "AVG PSNR:  17.918023654392787 AVG SSIM:  0.6512717264039176 AVG Loss:  0.22863908857107162\n",
            "-----Testing-----\n",
            "PSNR:  19.538666915893554 SSIM:  0.6857935428619385\n",
            "-----Model Saved-----\n",
            "We are in epoch: 31\n",
            "AVG PSNR:  18.277119840894425 AVG SSIM:  0.6649275124073029 AVG Loss:  0.21901077990021026\n",
            "We are in epoch: 32\n",
            "AVG PSNR:  19.646519660949707 AVG SSIM:  0.6811135730573109 AVG Loss:  0.20302396374089376\n",
            "We are in epoch: 33\n",
            "AVG PSNR:  18.653322219848633 AVG SSIM:  0.6743948374475751 AVG Loss:  0.21243489533662796\n",
            "We are in epoch: 34\n",
            "AVG PSNR:  18.00825834274292 AVG SSIM:  0.6784054466656276 AVG Loss:  0.2153324740273612\n",
            "We are in epoch: 35\n",
            "AVG PSNR:  18.516222408839635 AVG SSIM:  0.6899129152297974 AVG Loss:  0.20438461218561446\n",
            "-----Testing-----\n",
            "PSNR:  19.077812957763673 SSIM:  0.7020862579345704\n",
            "-----Model Saved-----\n",
            "We are in epoch: 36\n",
            "AVG PSNR:  18.733195032392228 AVG SSIM:  0.6533826547009605 AVG Loss:  0.22306162012474878\n",
            "We are in epoch: 37\n",
            "AVG PSNR:  19.126548835209437 AVG SSIM:  0.6766086050442287 AVG Loss:  0.2086841921721186\n",
            "We are in epoch: 38\n",
            "AVG PSNR:  18.080361706869944 AVG SSIM:  0.6758895814418793 AVG Loss:  0.21727648696729115\n",
            "We are in epoch: 39\n",
            "AVG PSNR:  17.386401789528982 AVG SSIM:  0.6691463461944035 AVG Loss:  0.22675954976252147\n",
            "We are in epoch: 40\n",
            "AVG PSNR:  17.523889201028005 AVG SSIM:  0.6795017846993038 AVG Loss:  0.21639609869037355\n",
            "-----Testing-----\n",
            "PSNR:  18.964042282104494 SSIM:  0.6710453748703002\n",
            "-----Model Saved-----\n",
            "We are in epoch: 41\n",
            "AVG PSNR:  19.227959632873535 AVG SSIM:  0.6973595746925899 AVG Loss:  0.19540114275046758\n",
            "We are in epoch: 42\n",
            "AVG PSNR:  18.449372495923722 AVG SSIM:  0.6737984035696302 AVG Loss:  0.21375219098159245\n",
            "We are in epoch: 43\n",
            "AVG PSNR:  19.220191751207626 AVG SSIM:  0.6839507903371539 AVG Loss:  0.2053255076919283\n",
            "We are in epoch: 44\n",
            "AVG PSNR:  18.162877559661865 AVG SSIM:  0.6925419781889234 AVG Loss:  0.20585339729275023\n",
            "We are in epoch: 45\n",
            "AVG PSNR:  17.88088035583496 AVG SSIM:  0.6720486921923501 AVG Loss:  0.2186766586133412\n",
            "-----Testing-----\n",
            "PSNR:  18.93881721496582 SSIM:  0.6866851449012756\n",
            "-----Model Saved-----\n",
            "We are in epoch: 46\n",
            "AVG PSNR:  18.624613216945104 AVG SSIM:  0.6822053704942975 AVG Loss:  0.20889945647546224\n",
            "We are in epoch: 47\n",
            "AVG PSNR:  18.674335752214706 AVG SSIM:  0.6846625528165272 AVG Loss:  0.20881850378853933\n",
            "We are in epoch: 48\n",
            "AVG PSNR:  18.560845034463064 AVG SSIM:  0.6848679738385337 AVG Loss:  0.2074674101812499\n",
            "We are in epoch: 49\n",
            "AVG PSNR:  18.738749299730575 AVG SSIM:  0.7052087954112461 AVG Loss:  0.19477773032018117\n",
            "We are in epoch: 50\n",
            "AVG PSNR:  18.87134987967355 AVG SSIM:  0.674224568264825 AVG Loss:  0.20949851082903997\n",
            "-----Testing-----\n",
            "PSNR:  20.133760070800783 SSIM:  0.6825345277786254\n",
            "-----Model Saved-----\n",
            "We are in epoch: 51\n",
            "AVG PSNR:  18.56340946469988 AVG SSIM:  0.6907403724534171 AVG Loss:  0.20531704702547618\n",
            "We are in epoch: 52\n",
            "AVG PSNR:  17.612215042114258 AVG SSIM:  0.6775218759264264 AVG Loss:  0.218029223382473\n",
            "We are in epoch: 53\n",
            "AVG PSNR:  17.514636312212264 AVG SSIM:  0.6625932965959821 AVG Loss:  0.22503669240645\n",
            "We are in epoch: 54\n",
            "AVG PSNR:  19.705834116254533 AVG SSIM:  0.6858251776014056 AVG Loss:  0.20011907815933228\n",
            "We are in epoch: 55\n",
            "AVG PSNR:  18.101982729775564 AVG SSIM:  0.6867876350879669 AVG Loss:  0.20909069797822408\n",
            "-----Testing-----\n",
            "PSNR:  18.343778228759767 SSIM:  0.7093041062355041\n",
            "-----Model Saved-----\n",
            "We are in epoch: 56\n",
            "AVG PSNR:  17.949300493512833 AVG SSIM:  0.6680376487118858 AVG Loss:  0.2214185744524002\n",
            "We are in epoch: 57\n",
            "AVG PSNR:  18.81773178918021 AVG SSIM:  0.6630169706685203 AVG Loss:  0.21831461787223816\n",
            "We are in epoch: 58\n",
            "AVG PSNR:  17.957287515912736 AVG SSIM:  0.6710388277258191 AVG Loss:  0.21682226019246237\n",
            "We are in epoch: 59\n",
            "AVG PSNR:  18.377232210976736 AVG SSIM:  0.692826645714896 AVG Loss:  0.20322342216968536\n",
            "We are in epoch: 60\n",
            "AVG PSNR:  19.263051441737584 AVG SSIM:  0.683857894369534 AVG Loss:  0.20203722161906107\n",
            "-----Testing-----\n",
            "PSNR:  20.681464004516602 SSIM:  0.7170467019081116\n",
            "-----Model Saved-----\n",
            "-----Best Model Saved-----\n",
            "We are in epoch: 61\n",
            "AVG PSNR:  19.538051468985422 AVG SSIM:  0.6912687846592495 AVG Loss:  0.19808575085231236\n",
            "We are in epoch: 62\n",
            "AVG PSNR:  18.706912108830043 AVG SSIM:  0.6655205211469105 AVG Loss:  0.21668791345187596\n",
            "We are in epoch: 63\n",
            "AVG PSNR:  18.656927381243026 AVG SSIM:  0.6939379445144108 AVG Loss:  0.20194586579288756\n",
            "We are in epoch: 64\n",
            "AVG PSNR:  17.90878507069179 AVG SSIM:  0.6833314597606659 AVG Loss:  0.21290787096534455\n",
            "We are in epoch: 65\n",
            "AVG PSNR:  17.478572709219797 AVG SSIM:  0.6631949033055987 AVG Loss:  0.22884926412786757\n",
            "-----Testing-----\n",
            "PSNR:  18.509561920166014 SSIM:  0.6950898885726928\n",
            "-----Model Saved-----\n",
            "We are in epoch: 66\n",
            "AVG PSNR:  17.90694216319493 AVG SSIM:  0.6556114256381989 AVG Loss:  0.22836996189185552\n",
            "We are in epoch: 67\n",
            "AVG PSNR:  19.982479776654923 AVG SSIM:  0.708614149263927 AVG Loss:  0.18589536952120916\n",
            "We are in epoch: 68\n",
            "AVG PSNR:  18.432607105800084 AVG SSIM:  0.6594236684697015 AVG Loss:  0.2204670969929014\n",
            "We are in epoch: 69\n",
            "AVG PSNR:  19.38400105067662 AVG SSIM:  0.7025055523429599 AVG Loss:  0.19383479867662703\n",
            "We are in epoch: 70\n",
            "AVG PSNR:  19.92077350616455 AVG SSIM:  0.6992808750697544 AVG Loss:  0.19185277393886022\n",
            "-----Testing-----\n",
            "PSNR:  20.122395706176757 SSIM:  0.7034095287322998\n",
            "-----Model Saved-----\n",
            "We are in epoch: 71\n",
            "AVG PSNR:  18.88602842603411 AVG SSIM:  0.6674118084566933 AVG Loss:  0.21417091148240225\n",
            "We are in epoch: 72\n",
            "AVG PSNR:  18.436977522713796 AVG SSIM:  0.6809365025588444 AVG Loss:  0.212479938353811\n",
            "We are in epoch: 73\n",
            "AVG PSNR:  18.461825234549387 AVG SSIM:  0.6957864931651524 AVG Loss:  0.2034901220883642\n",
            "We are in epoch: 74\n",
            "AVG PSNR:  19.484451838902064 AVG SSIM:  0.7111054531165532 AVG Loss:  0.18753602249281748\n",
            "We are in epoch: 75\n",
            "AVG PSNR:  17.62302201134818 AVG SSIM:  0.6890753635338375 AVG Loss:  0.21215851179191045\n",
            "-----Testing-----\n",
            "PSNR:  19.488721084594726 SSIM:  0.7039242744445801\n",
            "-----Model Saved-----\n",
            "We are in epoch: 76\n",
            "AVG PSNR:  18.278965950012207 AVG SSIM:  0.6958261259964534 AVG Loss:  0.2031219101377896\n",
            "We are in epoch: 77\n",
            "AVG PSNR:  20.791112967899867 AVG SSIM:  0.7024330496788025 AVG Loss:  0.18536254444292613\n",
            "We are in epoch: 78\n",
            "AVG PSNR:  20.25492995125907 AVG SSIM:  0.6996057331562042 AVG Loss:  0.19136794550078257\n",
            "We are in epoch: 79\n",
            "AVG PSNR:  19.092586381094797 AVG SSIM:  0.674332103558949 AVG Loss:  0.20805790168898447\n",
            "We are in epoch: 80\n",
            "AVG PSNR:  19.082860742296493 AVG SSIM:  0.680952787399292 AVG Loss:  0.2074097916483879\n",
            "-----Testing-----\n",
            "PSNR:  18.741376304626463 SSIM:  0.7175350427627564\n",
            "-----Model Saved-----\n",
            "We are in epoch: 81\n",
            "AVG PSNR:  20.24828107016427 AVG SSIM:  0.7022861753191266 AVG Loss:  0.18860423352037156\n",
            "We are in epoch: 82\n",
            "AVG PSNR:  18.61370141165597 AVG SSIM:  0.6947728097438812 AVG Loss:  0.20081333283867156\n",
            "We are in epoch: 83\n",
            "AVG PSNR:  19.45265769958496 AVG SSIM:  0.6875576376914978 AVG Loss:  0.20182802421706064\n",
            "We are in epoch: 84\n",
            "AVG PSNR:  18.851154463631765 AVG SSIM:  0.688742607831955 AVG Loss:  0.20616787991353444\n",
            "We are in epoch: 85\n",
            "AVG PSNR:  19.08697441646031 AVG SSIM:  0.693008371761867 AVG Loss:  0.20081548605646407\n",
            "-----Testing-----\n",
            "PSNR:  20.306410598754884 SSIM:  0.7201683998107911\n",
            "-----Model Saved-----\n",
            "We are in epoch: 86\n",
            "AVG PSNR:  19.19985968726022 AVG SSIM:  0.7043329860482898 AVG Loss:  0.19537992030382156\n",
            "We are in epoch: 87\n",
            "AVG PSNR:  19.27219799586705 AVG SSIM:  0.7000828172479358 AVG Loss:  0.19506944715976715\n",
            "We are in epoch: 88\n",
            "AVG PSNR:  18.99528012956892 AVG SSIM:  0.7095585933753422 AVG Loss:  0.1932910302920001\n",
            "We are in epoch: 89\n",
            "AVG PSNR:  18.80369131905692 AVG SSIM:  0.68501415848732 AVG Loss:  0.2043351105281285\n",
            "We are in epoch: 90\n",
            "AVG PSNR:  18.950521332877024 AVG SSIM:  0.6939433344772884 AVG Loss:  0.20242715626955032\n",
            "-----Testing-----\n",
            "PSNR:  19.013656234741212 SSIM:  0.700730848312378\n",
            "-----Model Saved-----\n",
            "We are in epoch: 91\n",
            "AVG PSNR:  17.646179948534286 AVG SSIM:  0.6641169232981545 AVG Loss:  0.22441008580582483\n",
            "We are in epoch: 92\n",
            "AVG PSNR:  18.585076808929443 AVG SSIM:  0.6761943016733442 AVG Loss:  0.21999828198126384\n",
            "We are in epoch: 93\n",
            "AVG PSNR:  18.583393505641393 AVG SSIM:  0.6942593242440905 AVG Loss:  0.2056028385247503\n",
            "We are in epoch: 94\n",
            "AVG PSNR:  19.829678262983048 AVG SSIM:  0.7019245879990714 AVG Loss:  0.19158718309232167\n",
            "We are in epoch: 95\n",
            "AVG PSNR:  19.287397725241526 AVG SSIM:  0.6931713649204799 AVG Loss:  0.19934457859822682\n",
            "-----Testing-----\n",
            "PSNR:  20.906573867797853 SSIM:  0.718733286857605\n",
            "-----Model Saved-----\n",
            "-----Best Model Saved-----\n",
            "We are in epoch: 96\n",
            "AVG PSNR:  18.73022515433175 AVG SSIM:  0.6801273993083409 AVG Loss:  0.2091217360326222\n",
            "We are in epoch: 97\n",
            "AVG PSNR:  17.59345769882202 AVG SSIM:  0.6966113533292498 AVG Loss:  0.20765738082783564\n",
            "We are in epoch: 98\n",
            "AVG PSNR:  18.891006469726562 AVG SSIM:  0.7017318606376648 AVG Loss:  0.19768633906330382\n",
            "We are in epoch: 99\n",
            "AVG PSNR:  18.48534713472639 AVG SSIM:  0.7016299111502511 AVG Loss:  0.20297990420034953\n",
            "We are in epoch: 100\n",
            "AVG PSNR:  18.368745395115443 AVG SSIM:  0.6769722742693765 AVG Loss:  0.21478730706231935\n",
            "-----Testing-----\n",
            "PSNR:  21.06637306213379 SSIM:  0.7433488368988037\n",
            "-----Model Saved-----\n",
            "-----Best Model Saved-----\n",
            "We are in epoch: 101\n",
            "AVG PSNR:  18.581386225564138 AVG SSIM:  0.6935827519212451 AVG Loss:  0.20561688712665013\n",
            "We are in epoch: 102\n",
            "AVG PSNR:  18.512293543134415 AVG SSIM:  0.7066733113356999 AVG Loss:  0.19669580566031591\n",
            "We are in epoch: 103\n",
            "AVG PSNR:  18.58085870742798 AVG SSIM:  0.7078841073172433 AVG Loss:  0.1949238862310137\n",
            "We are in epoch: 104\n",
            "AVG PSNR:  19.1889454296657 AVG SSIM:  0.6919971193586077 AVG Loss:  0.20145088114908763\n",
            "We are in epoch: 105\n",
            "AVG PSNR:  18.663455145699636 AVG SSIM:  0.7060698228222984 AVG Loss:  0.1960576974919864\n",
            "-----Testing-----\n",
            "PSNR:  19.393600463867188 SSIM:  0.7063170313835144\n",
            "-----Model Saved-----\n",
            "We are in epoch: 106\n",
            "AVG PSNR:  19.705939429146902 AVG SSIM:  0.7263257418360028 AVG Loss:  0.1792708688548633\n",
            "We are in epoch: 107\n",
            "AVG PSNR:  19.318744659423828 AVG SSIM:  0.678665280342102 AVG Loss:  0.20804255668606078\n",
            "We are in epoch: 108\n",
            "AVG PSNR:  19.709991250719344 AVG SSIM:  0.7008411714008876 AVG Loss:  0.19159074127674103\n",
            "We are in epoch: 109\n",
            "AVG PSNR:  18.66017028263637 AVG SSIM:  0.7058986467974526 AVG Loss:  0.19761533396584646\n",
            "We are in epoch: 110\n",
            "AVG PSNR:  19.175442014421737 AVG SSIM:  0.6782578315053668 AVG Loss:  0.20964480404342925\n",
            "-----Testing-----\n",
            "PSNR:  21.48096923828125 SSIM:  0.737470543384552\n",
            "-----Model Saved-----\n",
            "-----Best Model Saved-----\n",
            "We are in epoch: 111\n"
          ]
        }
      ],
      "source": [
        "# --- train --- #\n",
        "train_epoch = 500 # currently at 320 and should reach 820 after this\n",
        "best_psnr = 20.33\n",
        "\n",
        "TRAIN_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/O-HAZE/train/hazy/\"\n",
        "TRAIN_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/O-HAZE/train/GT/\"\n",
        "TEST_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/O-HAZE/test/hazy/\"\n",
        "TEST_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/O-HAZE/test/GT/\"\n",
        "\n",
        "# IMAGE_SIZE = (640,640)\n",
        "# IMAGE_SIZE = (512,512)\n",
        "IMAGE_SIZE = (384,384)\n",
        "# IMAGE_SIZE = (256,256)\n",
        "# RANDOM_CROP_SIZES = [256, 512]\n",
        "# TRAIN_BATCH_SIZE = 2\n",
        "TRAIN_BATCH_SIZE = 3\n",
        "#TRAIN_BATCH_SIZE = 5\n",
        "\n",
        "crop_size = [512,512]\n",
        "overlap_size = [256,256]\n",
        "\n",
        "VAL_BATCH_SIZE = 1\n",
        "TEST_BATCH_SIZE = 1\n",
        "NUM_WORKERS = 0\n",
        "SHUFFLE = True\n",
        "# --- output picture and check point --- #\n",
        "G_model_save_dir = \"/content/drive/MyDrive/Graduation Project/CANT_Haze_v10/weights/O-HAZE_BRB_DWT_RESBOTNCK_RESTBLOCK_DECONV_REFINE_XCEPTION.pth\"\n",
        "G_best_model_save_dir = \"/content/drive/MyDrive/Graduation Project/CANT_Haze_v10/weights/Best_O-HAZE_BRB_DWT_RESBOTNCK_RESTBLOCK_DECONV_REFINE_XCEPTION.pth\"\n",
        "# --- Gpu device --- #\n",
        "device_ids = [Id for Id in range(torch.cuda.device_count())]\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Define the network --- #\n",
        "MyEnsembleNet = fusion_net()\n",
        "# MyEnsembleNet = t_compile(MyEnsembleNet)\n",
        "# for name, param in MyEnsembleNet.named_parameters():\n",
        "#     if param.requires_grad and 'haze_density' in name:\n",
        "#         param.requires_grad = False\n",
        "# non_frozen_parameters = [p for p in MyEnsembleNet.parameters() if p.requires_grad]\n",
        "\n",
        "print('MyEnsembleNet parameters:', sum(param.numel() for param in MyEnsembleNet.parameters()))\n",
        "# print('Nonfrozen parameters:', sum(param.numel() for param in non_frozen_parameters))\n",
        "\n",
        "# --- Build optimizer --- #\n",
        "G_optimizer = torch.optim.Adam(MyEnsembleNet.parameters(), lr=0.0001)\n",
        "# G_optimizer = torch.optim.Adam(MyEnsembleNet.parameters(), lr=0.0001 * 0.5)\n",
        "# scheduler_G = torch.optim.lr_scheduler.MultiStepLR(G_optimizer, milestones=[3000, 5000, 6000], gamma=0.5)\n",
        "\n",
        "dataset = custom_dehaze_train_dataset(HAZY_path = TRAIN_HAZY_IMAGES_PATH, GT_path = TRAIN_GT_IMAGES_PATH, is_train = True, Image_Size = IMAGE_SIZE)\n",
        "                                      # random_crop_sizes = RANDOM_CROP_SIZES, random_crops = True)\n",
        "train_loader = DataLoader(dataset=dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=SHUFFLE)\n",
        "\n",
        "# val_dataset = CustomDataLoader(HAZY_path = VAL_HAZY_IMAGES_PATH,\n",
        "#                               GT_path = VAL_GT_IMAGES_PATH,\n",
        "#                               image_size = IMAGE_SIZE,\n",
        "#                               crop = True)\n",
        "\n",
        "# val_loader = DataLoader(val_dataset,\n",
        "#                           batch_size = VAL_BATCH_SIZE,\n",
        "#                           num_workers = NUM_WORKERS,\n",
        "#                           shuffle = False)\n",
        "\n",
        "# --- Load testing data --- #\n",
        "test_data = CustomDataLoader(HAZY_path = TEST_HAZY_IMAGES_PATH,\n",
        "                            GT_path = TEST_GT_IMAGES_PATH,\n",
        "                            # image_size = (768,1024),\n",
        "                            crop = False,\n",
        "                            resize = False)\n",
        "\n",
        "test_loader = DataLoader(test_data,\n",
        "                        batch_size = TEST_BATCH_SIZE,\n",
        "                        num_workers = NUM_WORKERS)\n",
        "\n",
        "MyEnsembleNet = MyEnsembleNet.to(device)\n",
        "\n",
        "# --- Load the network weight --- #\n",
        "try:\n",
        "    MyEnsembleNet.load_state_dict(torch.load(G_model_save_dir))\n",
        "    # MyEnsembleNet.load_state_dict(torch.load(\"/content/drive/MyDrive/Graduation Project/CANT_Haze_v10/weights/Best_NH_BRB_DWT_RESBOTNCK_RESTBLOCK_DECONV_REFINE_XCEPTION.pth\"), strict = False)\n",
        "    print('--- weight loaded ---')\n",
        "except:\n",
        "    print('--- no weight loaded ---')\n",
        "\n",
        "psnr = PSNR(data_range=1.0).to(device)\n",
        "ssim = SSIM(data_range=1.0).to(device)\n",
        "l1_loss = nn.L1Loss().to(device)\n",
        "# --- Start training --- #\n",
        "for epoch in range(train_epoch):\n",
        "    psnr_list = []\n",
        "    ssim_list = []\n",
        "    MyEnsembleNet.train()\n",
        "    avg_loss = 0\n",
        "    print(\"We are in epoch: \" + str(epoch+1))\n",
        "\n",
        "    for batch_idx, (hazy, clean) in enumerate(train_loader):\n",
        "            hazy = hazy.to(device)\n",
        "            clean = clean.to(device)\n",
        "            output = MyEnsembleNet(hazy)\n",
        "            l1_loss_ = l1_loss(output, clean)\n",
        "            ssim_ = ssim(output, clean)\n",
        "            psnr_ = psnr(output, clean)\n",
        "            ssim_loss_ = 1 - ssim_\n",
        "            # psnr_loss = 1 - psnr_ / 100\n",
        "            calc_ssim = ssim_.item()\n",
        "            calc_psnr = psnr_.item()\n",
        "            total_loss = (l1_loss_ +  ssim_loss_) / 2\n",
        "            # total_loss = (l1_loss_ +  ssim_loss_ + psnr_loss) / 3\n",
        "            avg_loss += total_loss.item()\n",
        "            MyEnsembleNet.zero_grad()\n",
        "            total_loss.backward()\n",
        "            G_optimizer.step()\n",
        "            # print('PSNR: ', calc_psnr, 'SSIM: ', calc_ssim, 'SSIM_Loss: ', ssim_loss_.item(), 'l1_loss: ', l1_loss_.item(), 'total_loss', total_loss.item())\n",
        "            psnr_list.append(calc_psnr)\n",
        "            ssim_list.append(calc_ssim)\n",
        "            # del hazy, clean, output\n",
        "            # gc.collect()\n",
        "\n",
        "    avr_psnr = sum(psnr_list) / len(psnr_list)\n",
        "    avr_ssim = sum(ssim_list) / len(ssim_list)\n",
        "    print('AVG PSNR: ', avr_psnr, 'AVG SSIM: ', avr_ssim, 'AVG Loss: ', avg_loss / len(psnr_list))\n",
        "\n",
        "    # with torch.inference_mode():\n",
        "    #     print(\"-----Validating-----\")\n",
        "    #     psnr_list = []\n",
        "    #     ssim_list = []\n",
        "    #     MyEnsembleNet.eval()\n",
        "\n",
        "    #     for batch_idx, (hazy, clean) in enumerate(val_loader):\n",
        "    #         for i in range(len(hazy)):\n",
        "    #             hazy[i] = hazy[i].to(device)\n",
        "    #             clean[i] = clean[i].to(device)\n",
        "    #             output = MyEnsembleNet(hazy[i])\n",
        "    #             calc_psnr = to_psnr(output, clean[i])\n",
        "    #             calc_ssim = to_ssim_skimage(output, clean[i])\n",
        "    #             psnr_list.extend(calc_psnr)\n",
        "    #             ssim_list.extend(calc_ssim)\n",
        "\n",
        "    # avr_psnr = sum(psnr_list) / len(psnr_list)\n",
        "    # avr_ssim = sum(ssim_list) / len(ssim_list)\n",
        "    # print('AVG PSNR: ', avr_psnr, 'AVG SSIM: ', avr_ssim)\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "            print(\"-----Testing-----\")\n",
        "            with torch.inference_mode():\n",
        "                psnr_list = []\n",
        "                ssim_list = []\n",
        "                MyEnsembleNet.eval()\n",
        "                for batch_idx, (hazy, clean, data_name) in enumerate(test_loader):\n",
        "                    clean = clean.to(device)\n",
        "                    hazy = hazy.to(device)\n",
        "                    frame_out = MyEnsembleNet(hazy)\n",
        "                    # if not os.path.exists('test/'):\n",
        "                    #     os.makedirs('test/')\n",
        "                    # imwrite(frame_out, 'test/' + ''.join(data_name) + '.png', range=(0, 1))\n",
        "                    imwrite(frame_out, '/content/drive/MyDrive/Graduation Project/CANT_Haze_v10/results_O-HAZE/' + ''.join(data_name) + '.png', range=(0, 1))\n",
        "                    psnr_list.append(psnr(frame_out, clean).item())\n",
        "                    ssim_list.append(ssim(frame_out, clean).item())\n",
        "            avr_psnr = sum(psnr_list) / len(psnr_list)\n",
        "            avr_ssim = sum(ssim_list) / len(ssim_list)\n",
        "            print('PSNR: ', avr_psnr, 'SSIM: ', avr_ssim)\n",
        "            torch.save(MyEnsembleNet.state_dict(), G_model_save_dir)\n",
        "            print(\"-----Model Saved-----\")\n",
        "            if(avr_psnr > best_psnr):\n",
        "                best_psnr = avr_psnr\n",
        "                torch.save(MyEnsembleNet.state_dict(), G_best_model_save_dir)\n",
        "                print(\"-----Best Model Saved-----\")\n",
        "\n",
        "print('Best PSNR: ', best_psnr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtp4e-geYQk5"
      },
      "source": [
        "# Testing (Full Dimensions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jivu2HeOYVwt"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "# VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/Test_Hazy/\"\n",
        "# VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/Test_GT/\"\n",
        "\n",
        "VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/valid_dense/HAZY/\"\n",
        "VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/valid_dense/GT/\"\n",
        "\n",
        "# VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/Real_compressed/\"\n",
        "# VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/Real_compressed/\"\n",
        "\n",
        "# VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NTIRE23/TEST/\"\n",
        "# VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NTIRE23/TEST/\"\n",
        "\n",
        "VAL_BATCH_SIZE = 1\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "# --- output picture and check point --- #\n",
        "# G_model_save_dir = \"/content/drive/MyDrive/Graduation Project/CANT_Haze_v8/weights/Best_model_weights.pth\"\n",
        "G_model_save_dir = \"/content/drive/MyDrive/Graduation Project/CANT_Haze_v10/weights/Best_DENSE_BRB_DWT_RESBOTNCK_RESTBLOCK_DECONV_REFINE_XCEPTION.pth\"\n",
        "# --- Gpu device --- #\n",
        "device_ids = [Id for Id in range(torch.cuda.device_count())]\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Define the network --- #\n",
        "MyEnsembleNet = fusion_net()\n",
        "print('MyEnsembleNet parameters:', sum(param.numel() for param in MyEnsembleNet.parameters()))\n",
        "\n",
        "# --- Load testing data --- #\n",
        "val_data = CustomDataLoader(HAZY_path = VAL_HAZY_IMAGES_PATH,\n",
        "                            GT_path = VAL_GT_IMAGES_PATH,\n",
        "                            # image_size = (720,1280),\n",
        "                            resize = False,\n",
        "                            crop = False)\n",
        "\n",
        "val_loader = DataLoader(val_data,\n",
        "                        batch_size = VAL_BATCH_SIZE,\n",
        "                        num_workers = NUM_WORKERS)\n",
        "\n",
        "MyEnsembleNet = MyEnsembleNet.to(device)\n",
        "\n",
        "# --- Load the network weight --- #\n",
        "try:\n",
        "    MyEnsembleNet.load_state_dict(torch.load(G_model_save_dir), strict = False)\n",
        "    # MyEnsembleNet = torch.load(\"/content/drive/MyDrive/Graduation Project/CANT_Haze_v8/weights2/full_model_test.pth\")\n",
        "    print('--- weight loaded ---')\n",
        "except:\n",
        "    print('--- no weight loaded ---')\n",
        "\n",
        "psnr = PSNR(data_range=1.0).to(device)\n",
        "ssim = SSIM(data_range=1.0).to(device)\n",
        "\n",
        "# --- Start training --- #\n",
        "print(\"-----Testing-----\")\n",
        "with torch.inference_mode():\n",
        "    psnr_list = []\n",
        "    ssim_list = []\n",
        "    avg_time = 0\n",
        "    MyEnsembleNet.eval()\n",
        "    for batch_idx, (hazy, clean, data_name) in enumerate(val_loader):\n",
        "        clean = clean.to(device)\n",
        "        hazy = hazy.to(device)\n",
        "        start = time.time()\n",
        "        frame_out = MyEnsembleNet(hazy)\n",
        "        end = time.time()\n",
        "        avg_time += (end - start) * 10 ** 3\n",
        "        if not os.path.exists('test3/'):\n",
        "            os.makedirs('test3/')\n",
        "        imwrite(frame_out, 'test3/' + ''.join(data_name) + '.png', range=(0, 1))\n",
        "        psnr_list.append(psnr(frame_out, clean).item())\n",
        "        ssim_list.append(ssim(frame_out, clean).item())\n",
        "avr_psnr = sum(psnr_list) / len(psnr_list)\n",
        "avr_ssim = sum(ssim_list) / len(ssim_list)\n",
        "avg_time /= len(psnr_list)\n",
        "print('PSNR: ', avr_psnr, 'SSIM: ', avr_ssim, 'Time: ', avg_time,'(ms)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C27YSofkk1jQ"
      },
      "source": [
        "# Testing (Custom Shaped Crops)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLfqsN5ak1jz"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "crop_size = [512,512]\n",
        "overlap_size = [256,256]\n",
        "\n",
        "VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/Test_Hazy/\"\n",
        "VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/Test_GT/\"\n",
        "\n",
        "# VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/Real_original/\"\n",
        "# VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/Real_original/\"\n",
        "\n",
        "# VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NTIRE23/TEST/\"\n",
        "# VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NTIRE23/TEST/\"\n",
        "\n",
        "VAL_BATCH_SIZE = 1\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "# --- output picture and check point --- #\n",
        "G_model_save_dir = \"/content/drive/MyDrive/Graduation Project/CANT_Haze_v10/weights/Best_NH_BRB_DWT_RESBOTNCK_RESTBLOCK_DECONV_REFINE_XCEPTION.pth\"\n",
        "# --- Gpu device --- #\n",
        "device_ids = [Id for Id in range(torch.cuda.device_count())]\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Define the network --- #\n",
        "MyEnsembleNet = fusion_net()\n",
        "print('MyEnsembleNet parameters:', sum(param.numel() for param in MyEnsembleNet.parameters()))\n",
        "\n",
        "# --- Load testing data --- #\n",
        "val_data = CustomDataLoader(HAZY_path = VAL_HAZY_IMAGES_PATH,\n",
        "                            GT_path = VAL_GT_IMAGES_PATH,\n",
        "                            # image_size = (720,1280),\n",
        "                            # resize = True,\n",
        "                            crop = False)\n",
        "\n",
        "val_loader = DataLoader(val_data,\n",
        "                        batch_size = VAL_BATCH_SIZE,\n",
        "                        num_workers = NUM_WORKERS)\n",
        "\n",
        "MyEnsembleNet = MyEnsembleNet.to(device)\n",
        "\n",
        "# --- Load the network weight --- #\n",
        "try:\n",
        "    MyEnsembleNet.load_state_dict(torch.load(G_model_save_dir))\n",
        "    print('--- weight loaded ---')\n",
        "except:\n",
        "    print('--- no weight loaded ---')\n",
        "\n",
        "psnr = PSNR(data_range=1.0).to(device)\n",
        "ssim = SSIM(data_range=1.0).to(device)\n",
        "\n",
        "# --- Start training --- #\n",
        "print(\"-----Testing-----\")\n",
        "with torch.inference_mode():\n",
        "    psnr_list = []\n",
        "    ssim_list = []\n",
        "    avg_time = 0\n",
        "    MyEnsembleNet.eval()\n",
        "    for batch_idx, (hazy, clean, data_name) in enumerate(val_loader):\n",
        "        clean = clean.to(device)\n",
        "        hazy = hazy.to(device)\n",
        "        start = time.time()\n",
        "        patch_list, crops_per_row, crops_per_col= custom_patchify(hazy, crop_size, overlap_size)\n",
        "\n",
        "        for i in range(crops_per_row):\n",
        "          for j in range(crops_per_col):\n",
        "            patch_list[i][j] = MyEnsembleNet(patch_list[i][j])\n",
        "\n",
        "        frame_out = custom_unpatchify(patch_list, overlap_size, crops_per_row, crops_per_col)\n",
        "\n",
        "        end = time.time()\n",
        "        avg_time += (end - start) * 10 ** 3\n",
        "        if not os.path.exists('test3/'):\n",
        "            os.makedirs('test3/')\n",
        "        imwrite(frame_out, 'test3/' + ''.join(data_name) + '.png', range=(0, 1))\n",
        "        psnr_list.append(psnr(frame_out, clean).item())\n",
        "        ssim_list.append(ssim(frame_out, clean).item())\n",
        "avr_psnr = sum(psnr_list) / len(psnr_list)\n",
        "avr_ssim = sum(ssim_list) / len(ssim_list)\n",
        "avg_time /= len(psnr_list)\n",
        "print('PSNR: ', avr_psnr, 'SSIM: ', avr_ssim, 'Time: ', avg_time,'(ms)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6hoIbyH3p_f"
      },
      "source": [
        "# Testing (Grid Shaped Crops)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4A2sF113p_h"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/Test_Hazy/\"\n",
        "VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NH-HAZE/Test_GT/\"\n",
        "\n",
        "# VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/Real_original/\"\n",
        "# VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/Real_original/\"\n",
        "\n",
        "# VAL_HAZY_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NTIRE23/TEST/\"\n",
        "# VAL_GT_IMAGES_PATH = \"/content/drive/MyDrive/Graduation Project/data/NTIRE23/TEST/\"\n",
        "\n",
        "VAL_BATCH_SIZE = 1\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "# --- output picture and check point --- #\n",
        "G_model_save_dir = \"/content/drive/MyDrive/Graduation Project/CANT_Haze_v10/weights/Best_NH_BRB_DWT_RESBOTNCK_RESTBLOCK_DECONV_REFINE_XCEPTION.pth\"\n",
        "# --- Gpu device --- #\n",
        "device_ids = [Id for Id in range(torch.cuda.device_count())]\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Define the network --- #\n",
        "MyEnsembleNet = fusion_net()\n",
        "print('MyEnsembleNet parameters:', sum(param.numel() for param in MyEnsembleNet.parameters()))\n",
        "\n",
        "# --- Load testing data --- #\n",
        "val_data = CustomDataLoader(HAZY_path = VAL_HAZY_IMAGES_PATH,\n",
        "                            GT_path = VAL_GT_IMAGES_PATH,\n",
        "                            # image_size = (720,1280),\n",
        "                            # resize = True,\n",
        "                            crop = False)\n",
        "\n",
        "val_loader = DataLoader(val_data,\n",
        "                        batch_size = VAL_BATCH_SIZE,\n",
        "                        num_workers = NUM_WORKERS)\n",
        "\n",
        "MyEnsembleNet = MyEnsembleNet.to(device)\n",
        "\n",
        "# --- Load the network weight --- #\n",
        "try:\n",
        "    MyEnsembleNet.load_state_dict(torch.load(G_model_save_dir))\n",
        "    print('--- weight loaded ---')\n",
        "except:\n",
        "    print('--- no weight loaded ---')\n",
        "\n",
        "psnr = PSNR(data_range=1.0).to(device)\n",
        "ssim = SSIM(data_range=1.0).to(device)\n",
        "\n",
        "# --- Start training --- #\n",
        "print(\"-----Testing-----\")\n",
        "with torch.inference_mode():\n",
        "    psnr_list = []\n",
        "    ssim_list = []\n",
        "    avg_time = 0\n",
        "    MyEnsembleNet.eval()\n",
        "    for batch_idx, (hazy, clean, data_name) in enumerate(val_loader):\n",
        "        clean = clean.to(device)\n",
        "        hazy = hazy.to(device)\n",
        "        start = time.time()\n",
        "        patch_list_T, patch_list_B=grid_patchify(hazy,2)\n",
        "\n",
        "        for x in range(len(patch_list_T)):\n",
        "            patch_list_T[x] = MyEnsembleNet(patch_list_T[x])[0]\n",
        "            patch_list_B[x] = MyEnsembleNet(patch_list_B[x])[0]\n",
        "\n",
        "        frame_out = unpatchify(patch_list_T,patch_list_B).unsqueeze(0)\n",
        "\n",
        "        end = time.time()\n",
        "        avg_time += (end - start) * 10 ** 3\n",
        "        if not os.path.exists('test3/'):\n",
        "            os.makedirs('test3/')\n",
        "        imwrite(frame_out, 'test3/' + ''.join(data_name) + '.png', range=(0, 1))\n",
        "        psnr_list.append(psnr(frame_out, clean).item())\n",
        "        ssim_list.append(ssim(frame_out, clean).item())\n",
        "avr_psnr = sum(psnr_list) / len(psnr_list)\n",
        "avr_ssim = sum(ssim_list) / len(ssim_list)\n",
        "avg_time /= len(psnr_list)\n",
        "print('PSNR: ', avr_psnr, 'SSIM: ', avr_ssim, 'Time: ', avg_time,'(ms)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMhVPFKaQQfW"
      },
      "source": [
        "# Edge Impulse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKtB4VOMQXsn"
      },
      "outputs": [],
      "source": [
        "# #ONNX_ML=1\n",
        "# import torch\n",
        "# !pip install onnx\n",
        "# # !pip install onnxruntime\n",
        "# # !pip install onnx_tf\n",
        "# import onnx\n",
        "# # from onnx_tf.backend import prepare\n",
        "# # import onnxruntime as ort\n",
        "\n",
        "# G_model_save_dir = \"/content/drive/MyDrive/Graduation Project/CANT_Haze_v9/weights/Best_NH_BRB_DWT_RESBOTNCK_RESTBLOCK_LEAKYRELU_DEEP.pth\"\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# trained_model = CANT_HAZE().to(device)\n",
        "# trained_model.load_state_dict(torch.load(G_model_save_dir))\n",
        "# trained_model.eval()\n",
        "# with torch.inference_mode():\n",
        "\n",
        "#   dummy_input = torch.randn(3, 1200, 1600).unsqueeze(0).to(device) # define a random input example image\n",
        "\n",
        "#   torch.onnx.export(trained_model,\n",
        "#                     dummy_input,\n",
        "#                     \"fixed_model_opv9.onnx\",\n",
        "#                     export_params=True,\n",
        "#                     do_constant_folding=True,\n",
        "#                     opset_version=9,)\n",
        "#                     # input_names=['input'],\n",
        "#                     # output_names=['output'],\n",
        "#                     # dynamic_axes={'input': [2, 3], 'output' : [2, 3]}) # convert pytorch to ONNX\n",
        "\n",
        "#   model = onnx.load('fixed_model_opv9.onnx') # Load ONNX model\n",
        "\n",
        "#   # model = ort.InferenceSession(\"model.onnx\") # Load ONNX model using onnx runtime\n",
        "\n",
        "# # model = prepare(model) # convert ONNX to tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYX91HtwlwA4"
      },
      "outputs": [],
      "source": [
        "# !pip install edgeimpulse\n",
        "# import edgeimpulse as ei\n",
        "# model = onnx.load('model.onnx') # Load ONNX model\n",
        "\n",
        "# ei.API_KEY = \"ei_385eba63d6f91256bf2df2cbb07cb00a622b6faad386dfdffb796bac8fcf1c71\"\n",
        "# for device_name in ei.model.list_profile_devices():\n",
        "#   print(f\"------------ Profiling for device {device_name} ------------\")\n",
        "#   try:\n",
        "\n",
        "#     profile = ei.model.profile(model = model, device = device_name)\n",
        "#     print(profile.summary())\n",
        "#   except Exception as e:\n",
        "#     print(f\"Could not profile {device_name}: {e}\")\n",
        "#   print(\"------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ps5_BN4GA5j"
      },
      "source": [
        "# Weight Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8AL80eb0NK7"
      },
      "source": [
        "## Edit weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJHnUevBWaBI"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from collections import OrderedDict\n",
        "\n",
        "# # load the model\n",
        "# model = torch.load('/content/NH_MSRB_DWT2D.pth')\n",
        "\n",
        "# # separate backbone layers\n",
        "# new_state_dict = {}\n",
        "# for name, param in model.items():\n",
        "#     # print(name)\n",
        "#     if 'dwt.layer3' not in name and 'encoder.SK' not in name:\n",
        "#         #extract certain name\n",
        "#         # new_state_dict[name.replace('dwt_branch.','')] = param\n",
        "#         #remove certain parameter\n",
        "#         new_state_dict[name] = param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4v1B2ro0Pnt"
      },
      "source": [
        "## Save weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42RmIPwhF_5i"
      },
      "outputs": [],
      "source": [
        "# new_state_dict = OrderedDict(new_state_dict)\n",
        "\n",
        "# # save the new model which has only backbone layers, only uncomment when needed\n",
        "# ###torch.save(new_state_dict, '/content/NH_MSRB_DWT2D.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "fH4eeCUCWqCg",
        "4fDqJ3Xc6oo1",
        "QbwC6EjqVCnR",
        "uc_EUNSWpnUB",
        "ZmmteUSxpkiX",
        "D_-RV6anMq6l",
        "35Bzl6FB2gFf",
        "-ECumqlC2tNX",
        "8kzgsmWy2oOE",
        "DPvVSY7546O4",
        "vrfC9Co5vJ5G",
        "UT7XdDldATtJ",
        "db8pIBHce7Tq",
        "DDgYd9Dygr8q",
        "o8W3NwOJ7EIL",
        "aVS40dtjEcb7",
        "dabbMp9rYnwm",
        "iCahn0Dx7juC",
        "2RjStUdoC3Cy",
        "UxFw0ett7skh",
        "nX6y0TcI7u1N",
        "R2Qbmq-U7wj0",
        "rtp4e-geYQk5",
        "C27YSofkk1jQ",
        "B6hoIbyH3p_f",
        "sMhVPFKaQQfW",
        "_Ps5_BN4GA5j"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}